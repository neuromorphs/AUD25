{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f8dcdc3f-bf4f-4554-9a67-2d48262955e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import mtrf\n",
    "import librosa\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from IPython.display import Audio, display\n",
    "from sklearn.cross_decomposition import CCA\n",
    "\n",
    "from scipy.io import loadmat\n",
    "from scipy import linalg\n",
    "from scipy import stats\n",
    "from scipy.signal import hilbert, resample\n",
    "from scipy.stats import zscore, pearsonr\n",
    "\n",
    "from mtrf.model import TRF\n",
    "from sklearn.cross_decomposition import CCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "593b6220-8568-44af-91ca-5bc99f048e98",
   "metadata": {},
   "outputs": [],
   "source": [
    "def lag_generator_new(r, lags):\n",
    "    '''\n",
    "    Args:\n",
    "      r: [time, neurons]\n",
    "      \n",
    "    Return\n",
    "      out: [time, neuron*lags]\n",
    "    \n",
    "    '''\n",
    "    lags = list(range(lags[0], lags[1]+1))\n",
    "    out = np.zeros([r.shape[0], r.shape[1]*len(lags)])\n",
    "    r = np.pad(r, ((0,len(lags)),(0,0)), 'constant')\n",
    "\n",
    "    r_lag_list = []\n",
    "    \n",
    "    for lag in lags:\n",
    "        t1 = np.roll(r, lag, axis=0)\n",
    "        if lag < 0:\n",
    "            t1[lag-1:, :] = 0\n",
    "        else:\n",
    "            t1[:lag, :] = 0\n",
    "            \n",
    "        r_lag_list.append(t1[:out.shape[0], :])\n",
    "        \n",
    "    out = np.concatenate(r_lag_list, axis=1)\n",
    "    \n",
    "    return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "11f7a4bc-5ee0-4da0-9007-dd26a2d7031d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#trials = os.listdir('../../../Data/Cindy/Preprocessed/preprocessed_mixed_new')\n",
    "#trials.remove('cindy_mixed_pp_record.csv')\n",
    "\n",
    "# folder_name = '../../../Data/Cindy/Preprocessed/preprocessed_mixed_01_15Hz'\n",
    "# trials = os.listdir(folder_name)\n",
    "# trials = [item for item in trials if item not in ['cindy_mixed_pp_record.csv','.ipynb_checkpoints.mat','.ipynb_checkpoints']]\n",
    "\n",
    "\n",
    "folder_name = '../../../Data/Samet/Preprocessed/preprocessed_mixed_01_15Hz'\n",
    "trials = os.listdir(folder_name)\n",
    "trials = [item for item in trials if item not in ['multi1_pp_record.csv','multi2_pp_record.csv','multi3_pp_record.csv','multi4_pp_record.csv','.ipynb_checkpoints.mat','.ipynb_checkpoints']]\n",
    "\n",
    "fs_eeg = 128\n",
    "\n",
    "lags_neuro = [-40, 10]\n",
    "lags_stim = [-10, 10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "52b09784-ebeb-4cb1-978f-4d83ce2d32c2",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "samet_Music_4.mat\n",
      "samet_Music_27.mat\n",
      "samet_Music_33.mat\n",
      "samet_Music_32.mat\n",
      "samet_Music_26.mat\n",
      "samet_Music_5.mat\n",
      "samet_Music_7.mat\n",
      "samet_Music_18.mat\n",
      "samet_Music_30.mat\n",
      "samet_Music_24.mat\n",
      "samet_Music_25.mat\n",
      "samet_Music_31.mat\n",
      "samet_Music_19.mat\n",
      "samet_Music_6.mat\n",
      "samet_Music_2.mat\n",
      "samet_Music_35.mat\n",
      "samet_Music_21.mat\n",
      "samet_Music_20.mat\n",
      "samet_Music_34.mat\n",
      "samet_Music_3.mat\n",
      "samet_Music_1.mat\n",
      "samet_Music_22.mat\n",
      "samet_Music_36.mat\n",
      "samet_Music_37.mat\n",
      "samet_Music_23.mat\n",
      "samet_Music_0.mat\n",
      "samet_Speech_34.mat\n",
      "samet_Speech_20.mat\n",
      "samet_Speech_0.mat\n",
      "samet_Speech_1.mat\n",
      "samet_Speech_21.mat\n",
      "samet_Speech_35.mat\n",
      "samet_Speech_23.mat\n",
      "samet_Speech_37.mat\n",
      "samet_Speech_3.mat\n",
      "samet_Speech_2.mat\n",
      "samet_Speech_36.mat\n",
      "samet_Speech_22.mat\n",
      "samet_Speech_26.mat\n",
      "samet_Speech_32.mat\n",
      "samet_Speech_6.mat\n",
      "samet_Speech_7.mat\n",
      "samet_Speech_33.mat\n",
      "samet_Speech_27.mat\n",
      "samet_Speech_31.mat\n",
      "samet_Speech_25.mat\n",
      "samet_Speech_19.mat\n",
      "samet_Speech_5.mat\n",
      "samet_Speech_4.mat\n",
      "samet_Speech_18.mat\n",
      "samet_Speech_24.mat\n",
      "samet_Speech_30.mat\n",
      "samet_Speech_29.mat\n",
      "samet_Speech_15.mat\n",
      "samet_Speech_9.mat\n",
      "samet_Speech_8.mat\n",
      "samet_Speech_14.mat\n",
      "samet_Speech_28.mat\n",
      "samet_Speech_16.mat\n",
      "samet_Speech_17.mat\n",
      "samet_Speech_13.mat\n",
      "samet_Speech_12.mat\n",
      "samet_Speech_10.mat\n",
      "samet_Speech_11.mat\n",
      "samet_Music_12.mat\n",
      "samet_Music_13.mat\n",
      "samet_Music_11.mat\n",
      "samet_Music_10.mat\n",
      "samet_Music_14.mat\n",
      "samet_Music_28.mat\n",
      "samet_Music_29.mat\n",
      "samet_Music_15.mat\n",
      "samet_Music_8.mat\n",
      "samet_Music_17.mat\n",
      "samet_Music_16.mat\n",
      "samet_Music_9.mat\n"
     ]
    }
   ],
   "source": [
    "speech_eeg_all = []\n",
    "speech_att_env_all = []\n",
    "speech_unatt_env_all = []\n",
    "\n",
    "for trial in trials:\n",
    "    print(trial)\n",
    "    data = loadmat(os.path.join(folder_name,trial))\n",
    "    \n",
    "    if data['stim_attended'][0] == 'Speech':\n",
    "        att_stim,fs_audio = librosa.load(os.path.join(\"../../../Stimuli/Cindy/speech_only_short_22khz\",f\"{data['stimuli_speech'][0]}\"+'.wav'))\n",
    "        unatt_stim,fs_audio = librosa.load(os.path.join(\"../../../Stimuli/Cindy/piano_only_long_cropped_22khz\",f\"{data['stimuli_music'][0]}\"+'.wav'))\n",
    "    elif data['stim_attended'][0] == 'Music':\n",
    "        continue\n",
    "        att_stim,fs_audio = librosa.load(os.path.join(\"../../../Stimuli/Cindy/piano_only_long_cropped_22khz\",f\"{data['stimuli_music'][0]}\"+'.wav'))\n",
    "        unatt_stim,fs_audio = librosa.load(os.path.join(\"../../../Stimuli/Cindy/speech_only_short_22khz\",f\"{data['stimuli_speech'][0]}\"+'.wav'))\n",
    "        \n",
    "    \n",
    "    if data['stim_attended_pos'][0] == 'FirstHalfAttend':\n",
    "        att_stim = att_stim[:int(len(att_stim)/2)]\n",
    "        unatt_stim = unatt_stim[:int(len(unatt_stim)/2)]\n",
    "    elif data['stim_attended_pos'][0] == 'SecondHalfAttend':\n",
    "        att_stim = att_stim[int(len(att_stim)/2):]\n",
    "        unatt_stim = unatt_stim[int(len(unatt_stim)/2):]\n",
    "    \n",
    "    # display(Audio(att_stim,rate=fs_audio))\n",
    "    # display(Audio(unatt_stim,rate=fs_audio))\n",
    "    \n",
    "    att_env = np.abs(hilbert((att_stim)))\n",
    "    unatt_env = np.abs(hilbert((unatt_stim)))\n",
    "    \n",
    "    duration_sec = len(att_env) / fs_audio\n",
    "    n_target_samples = int(duration_sec * fs_eeg)\n",
    "    att_env = np.expand_dims(resample(att_env, n_target_samples),axis=0)\n",
    "    \n",
    "    duration_sec = len(unatt_env) / fs_audio\n",
    "    n_target_samples = int(duration_sec * fs_eeg)\n",
    "    unatt_env = np.expand_dims(resample(unatt_env, n_target_samples),axis=0)\n",
    "\n",
    "    speech_eeg_all.append(data['eeg_data'])\n",
    "    speech_att_env_all.append(att_env)\n",
    "    speech_unatt_env_all.append(unatt_env)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "945c4789-f5bb-4ada-827b-025c3a3b4195",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "samet_Music_4.mat\n",
      "samet_Music_27.mat\n",
      "samet_Music_33.mat\n",
      "samet_Music_32.mat\n",
      "samet_Music_26.mat\n",
      "samet_Music_5.mat\n",
      "samet_Music_7.mat\n",
      "samet_Music_18.mat\n",
      "samet_Music_30.mat\n",
      "samet_Music_24.mat\n",
      "samet_Music_25.mat\n",
      "samet_Music_31.mat\n",
      "samet_Music_19.mat\n",
      "samet_Music_6.mat\n",
      "samet_Music_2.mat\n",
      "samet_Music_35.mat\n",
      "samet_Music_21.mat\n",
      "samet_Music_20.mat\n",
      "samet_Music_34.mat\n",
      "samet_Music_3.mat\n",
      "samet_Music_1.mat\n",
      "samet_Music_22.mat\n",
      "samet_Music_36.mat\n",
      "samet_Music_37.mat\n",
      "samet_Music_23.mat\n",
      "samet_Music_0.mat\n",
      "samet_Speech_34.mat\n",
      "samet_Speech_20.mat\n",
      "samet_Speech_0.mat\n",
      "samet_Speech_1.mat\n",
      "samet_Speech_21.mat\n",
      "samet_Speech_35.mat\n",
      "samet_Speech_23.mat\n",
      "samet_Speech_37.mat\n",
      "samet_Speech_3.mat\n",
      "samet_Speech_2.mat\n",
      "samet_Speech_36.mat\n",
      "samet_Speech_22.mat\n",
      "samet_Speech_26.mat\n",
      "samet_Speech_32.mat\n",
      "samet_Speech_6.mat\n",
      "samet_Speech_7.mat\n",
      "samet_Speech_33.mat\n",
      "samet_Speech_27.mat\n",
      "samet_Speech_31.mat\n",
      "samet_Speech_25.mat\n",
      "samet_Speech_19.mat\n",
      "samet_Speech_5.mat\n",
      "samet_Speech_4.mat\n",
      "samet_Speech_18.mat\n",
      "samet_Speech_24.mat\n",
      "samet_Speech_30.mat\n",
      "samet_Speech_29.mat\n",
      "samet_Speech_15.mat\n",
      "samet_Speech_9.mat\n",
      "samet_Speech_8.mat\n",
      "samet_Speech_14.mat\n",
      "samet_Speech_28.mat\n",
      "samet_Speech_16.mat\n",
      "samet_Speech_17.mat\n",
      "samet_Speech_13.mat\n",
      "samet_Speech_12.mat\n",
      "samet_Speech_10.mat\n",
      "samet_Speech_11.mat\n",
      "samet_Music_12.mat\n",
      "samet_Music_13.mat\n",
      "samet_Music_11.mat\n",
      "samet_Music_10.mat\n",
      "samet_Music_14.mat\n",
      "samet_Music_28.mat\n",
      "samet_Music_29.mat\n",
      "samet_Music_15.mat\n",
      "samet_Music_8.mat\n",
      "samet_Music_17.mat\n",
      "samet_Music_16.mat\n",
      "samet_Music_9.mat\n"
     ]
    }
   ],
   "source": [
    "music_eeg_all = []\n",
    "music_att_env_all = []\n",
    "music_unatt_env_all = []\n",
    "\n",
    "for trial in trials:\n",
    "    print(trial)\n",
    "    data = loadmat(os.path.join(folder_name,trial))\n",
    "    \n",
    "    if data['stim_attended'][0] == 'Speech':\n",
    "        continue\n",
    "        att_stim,fs_audio = librosa.load(os.path.join(\"../../../Stimuli/Cindy/speech_only_short_22khz\",f\"{data['stimuli_speech'][0]}\"+'.wav'))\n",
    "        unatt_stim,fs_audio = librosa.load(os.path.join(\"../../../Stimuli/Cindy/piano_only_long_cropped_22khz\",f\"{data['stimuli_music'][0]}\"+'.wav'))\n",
    "    elif data['stim_attended'][0] == 'Music':\n",
    "        att_stim,fs_audio = librosa.load(os.path.join(\"../../../Stimuli/Cindy/piano_only_long_cropped_22khz\",f\"{data['stimuli_music'][0]}\"+'.wav'))\n",
    "        unatt_stim,fs_audio = librosa.load(os.path.join(\"../../../Stimuli/Cindy/speech_only_short_22khz\",f\"{data['stimuli_speech'][0]}\"+'.wav'))\n",
    "        \n",
    "    \n",
    "    if data['stim_attended_pos'][0] == 'FirstHalfAttend':\n",
    "        att_stim = att_stim[:int(len(att_stim)/2)]\n",
    "        unatt_stim = unatt_stim[:int(len(unatt_stim)/2)]\n",
    "    elif data['stim_attended_pos'][0] == 'SecondHalfAttend':\n",
    "        att_stim = att_stim[int(len(att_stim)/2):]\n",
    "        unatt_stim = unatt_stim[int(len(unatt_stim)/2):]\n",
    "    \n",
    "    # display(Audio(att_stim,rate=fs_audio))\n",
    "    # display(Audio(unatt_stim,rate=fs_audio))\n",
    "    \n",
    "    att_env = np.abs(hilbert((att_stim)))\n",
    "    unatt_env = np.abs(hilbert((unatt_stim)))\n",
    "    \n",
    "    duration_sec = len(att_env) / fs_audio\n",
    "    n_target_samples = int(duration_sec * fs_eeg)\n",
    "    att_env = np.expand_dims(resample(att_env, n_target_samples),axis=0)\n",
    "    \n",
    "    duration_sec = len(unatt_env) / fs_audio\n",
    "    n_target_samples = int(duration_sec * fs_eeg)\n",
    "    unatt_env = np.expand_dims(resample(unatt_env, n_target_samples),axis=0)\n",
    "\n",
    "    music_eeg_all.append(data['eeg_data'])\n",
    "    music_att_env_all.append(att_env)\n",
    "    music_unatt_env_all.append(unatt_env)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5c010a40-7c8f-496f-9f68-cb655fdc28ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# speech_eeg = np.concatenate(speech_eeg_all,axis=1).T\n",
    "# speech_stim_att = np.concatenate(speech_att_env_all,axis=1).T\n",
    "# speech_stim_unatt = np.concatenate(speech_unatt_env_all,axis=1).T\n",
    "# speech_eeg = zscore(speech_eeg, axis=0)\n",
    "# speech_stim_att = zscore(speech_stim_att, axis=0)\n",
    "# speech_stim_unatt = zscore(speech_stim_unatt, axis=0)\n",
    "\n",
    "# music_eeg = np.concatenate(music_eeg_all,axis=1).T\n",
    "# music_stim_att = np.concatenate(music_att_env_all,axis=1).T\n",
    "# music_stim_unatt = np.concatenate(music_unatt_env_all,axis=1).T\n",
    "# music_eeg = zscore(music_eeg, axis=0)\n",
    "# music_stim_att = zscore(music_stim_att, axis=0)\n",
    "# music_stim_unatt = zscore(music_stim_unatt, axis=0)\n",
    "\n",
    "speech_eeg = np.concatenate(speech_eeg_all,axis=1).T\n",
    "speech_stim_att = np.concatenate(speech_att_env_all,axis=1).T\n",
    "speech_stim_unatt = np.concatenate(speech_unatt_env_all,axis=1).T\n",
    "\n",
    "# speech_eeg_mean = (np.mean(speech_eeg,axis=0))\n",
    "# speech_eeg_std = (np.std(speech_eeg,axis=0))\n",
    "# speech_stim_att_mean = (np.mean(speech_stim_att,axis=0))\n",
    "# speech_stim_att_std = (np.std(speech_stim_att,axis=0))\n",
    "# speech_stim_unatt_mean = (np.mean(speech_stim_unatt,axis=0))\n",
    "# speech_stim_unatt_std = (np.std(speech_stim_unatt,axis=0))\n",
    "\n",
    "music_eeg = np.concatenate(music_eeg_all,axis=1).T\n",
    "music_stim_att = np.concatenate(music_att_env_all,axis=1).T\n",
    "music_stim_unatt = np.concatenate(music_unatt_env_all,axis=1).T\n",
    "\n",
    "# music_eeg_mean = (np.mean(music_eeg,axis=0))\n",
    "# music_eeg_std = (np.std(music_eeg,axis=0))\n",
    "# music_stim_att_mean = (np.mean(music_stim_att,axis=0))\n",
    "# music_stim_att_std = (np.std(music_stim_att,axis=0))\n",
    "# music_stim_unatt_mean = (np.mean(music_stim_unatt,axis=0))\n",
    "# music_stim_unatt_std = (np.std(music_stim_unatt,axis=0))\n",
    "\n",
    "eeg_mean = np.mean(np.concatenate((speech_eeg,music_eeg),axis = 0),axis=0)\n",
    "eeg_std = np.std(np.concatenate((speech_eeg,music_eeg),axis = 0),axis=0)\n",
    "\n",
    "stim_mean = np.mean(np.concatenate((speech_stim_att,speech_stim_unatt,music_stim_att,music_stim_unatt),axis = 0),axis=0)\n",
    "stim_std = np.std(np.concatenate((speech_stim_att,speech_stim_unatt,music_stim_att,music_stim_unatt),axis = 0),axis=0)\n",
    "\n",
    "speech_eeg = (speech_eeg-eeg_mean)/eeg_std\n",
    "music_eeg = (music_eeg-eeg_mean)/eeg_std\n",
    "\n",
    "speech_stim_att = (speech_stim_att-stim_mean)/stim_std\n",
    "speech_stim_unatt = (speech_stim_unatt-stim_mean)/stim_std\n",
    "music_stim_att = (music_stim_att-stim_mean)/stim_std\n",
    "music_stim_unatt = (music_stim_unatt-stim_mean)/stim_std\n",
    "\n",
    "\n",
    "# speech_eeg = zscore(speech_eeg, axis=0)\n",
    "# speech_stim_att = zscore(speech_stim_att, axis=0)\n",
    "# speech_stim_unatt = zscore(speech_stim_unatt, axis=0)\n",
    "\n",
    "# music_eeg = zscore(music_eeg, axis=0)\n",
    "# music_stim_att = zscore(music_stim_att, axis=0)\n",
    "# music_stim_unatt = zscore(music_stim_unatt, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a5d274ec-1266-493b-999d-9305854a7530",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Split 1\n",
      "Train: 0.143\n",
      "Attended Speech: 0.023\n",
      "Unattended Speech: 0.017\n",
      "Attended Music: 0.071\n",
      "Unattended Music: 0.028\n",
      "Split 2\n",
      "Train: 0.133\n",
      "Attended Speech: 0.032\n",
      "Unattended Speech: -0.002\n",
      "Attended Music: 0.049\n",
      "Unattended Music: 0.026\n",
      "Split 3\n",
      "Train: 0.138\n",
      "Attended Speech: -0.0\n",
      "Unattended Speech: 0.023\n",
      "Attended Music: 0.056\n",
      "Unattended Music: 0.064\n",
      "Split 4\n",
      "Train: 0.142\n",
      "Attended Speech: 0.036\n",
      "Unattended Speech: 0.012\n",
      "Attended Music: 0.076\n",
      "Unattended Music: 0.083\n",
      "Split 5\n",
      "Train: 0.137\n",
      "Attended Speech: 0.037\n",
      "Unattended Speech: 0.025\n",
      "Attended Music: 0.069\n",
      "Unattended Music: 0.07\n",
      "Split 6\n",
      "Train: 0.137\n",
      "Attended Speech: 0.042\n",
      "Unattended Speech: 0.032\n",
      "Attended Music: 0.087\n",
      "Unattended Music: 0.013\n",
      "Split 7\n",
      "Train: 0.14\n",
      "Attended Speech: 0.012\n",
      "Unattended Speech: 0.065\n",
      "Attended Music: 0.091\n",
      "Unattended Music: 0.033\n",
      "Split 8\n",
      "Train: 0.146\n",
      "Attended Speech: -0.011\n",
      "Unattended Speech: 0.014\n",
      "Attended Music: 0.021\n",
      "Unattended Music: 0.021\n",
      "Split 9\n",
      "Train: 0.14\n",
      "Attended Speech: 0.005\n",
      "Unattended Speech: 0.014\n",
      "Attended Music: 0.063\n",
      "Unattended Music: 0.031\n",
      "Split 10\n",
      "Train: 0.137\n",
      "Attended Speech: 0.053\n",
      "Unattended Speech: -0.001\n",
      "Attended Music: 0.072\n",
      "Unattended Music: -0.003\n",
      "Split 11\n",
      "Train: 0.142\n",
      "Attended Speech: -0.013\n",
      "Unattended Speech: -0.02\n",
      "Attended Music: 0.074\n",
      "Unattended Music: 0.055\n",
      "Split 12\n",
      "Train: 0.141\n",
      "Attended Speech: 0.066\n",
      "Unattended Speech: -0.005\n",
      "Attended Music: 0.094\n",
      "Unattended Music: 0.01\n",
      "Split 13\n",
      "Train: 0.14\n",
      "Attended Speech: 0.081\n",
      "Unattended Speech: 0.043\n",
      "Attended Music: 0.071\n",
      "Unattended Music: 0.05\n",
      "Split 14\n",
      "Train: 0.142\n",
      "Attended Speech: 0.067\n",
      "Unattended Speech: 0.007\n",
      "Attended Music: 0.05\n",
      "Unattended Music: 0.01\n",
      "Split 15\n",
      "Train: 0.147\n",
      "Attended Speech: 0.049\n",
      "Unattended Speech: -0.005\n",
      "Attended Music: 0.011\n",
      "Unattended Music: 0.044\n",
      "Split 16\n",
      "Train: 0.141\n",
      "Attended Speech: 0.047\n",
      "Unattended Speech: 0.045\n",
      "Attended Music: 0.076\n",
      "Unattended Music: 0.032\n",
      "Split 17\n",
      "Train: 0.142\n",
      "Attended Speech: 0.023\n",
      "Unattended Speech: 0.031\n",
      "Attended Music: 0.031\n",
      "Unattended Music: 0.039\n",
      "Split 18\n",
      "Train: 0.143\n",
      "Attended Speech: 0.051\n",
      "Unattended Speech: 0.022\n",
      "Attended Music: 0.057\n",
      "Unattended Music: 0.011\n",
      "Split 19\n",
      "Train: 0.143\n",
      "Attended Speech: 0.046\n",
      "Unattended Speech: 0.024\n",
      "Attended Music: 0.029\n",
      "Unattended Music: 0.001\n",
      "Split 20\n",
      "Train: 0.136\n",
      "Attended Speech: 0.014\n",
      "Unattended Speech: 0.079\n",
      "Attended Music: 0.085\n",
      "Unattended Music: 0.063\n",
      "Average Training Correlation: 0.14044388577573924\n",
      "Average Attended Speech Test Correlation: 0.032967772157962046\n",
      "Average Unttended Speech Test Correlation: 0.02091701493912248\n",
      "Average Attended Music Test Correlation: 0.061638973857888045\n",
      "Average Unttended Music Test Correlation: 0.03399504068299472\n"
     ]
    }
   ],
   "source": [
    "train_corrs = []\n",
    "speech_att_test_corrs = []\n",
    "speech_unatt_test_corrs = []\n",
    "music_att_test_corrs = []\n",
    "music_unatt_test_corrs = []\n",
    "\n",
    "\n",
    "speech_sample_len = speech_eeg.shape[0]\n",
    "music_sample_len = music_eeg.shape[0]\n",
    "\n",
    "k_cv = 20\n",
    "for i in range(k_cv):\n",
    "    print(f'Split {i+1}')\n",
    "\n",
    "    #Train Test Split\n",
    "    \n",
    "    speech_eeg_test = speech_eeg[i*(round(speech_sample_len/k_cv)):(i+1)*(round(speech_sample_len/k_cv)),:]\n",
    "    speech_stim_att_test = speech_stim_att[i*(round(speech_sample_len/k_cv)):(i+1)*(round(speech_sample_len/k_cv)),:]\n",
    "    speech_stim_unatt_test = speech_stim_unatt[i*(round(speech_sample_len/k_cv)):(i+1)*(round(speech_sample_len/k_cv)),:]\n",
    "\n",
    "    music_eeg_test = music_eeg[i*(round(music_sample_len/k_cv)):(i+1)*(round(music_sample_len/k_cv)),:]\n",
    "    music_stim_att_test = music_stim_att[i*(round(music_sample_len/k_cv)):(i+1)*(round(music_sample_len/k_cv)),:]\n",
    "    music_stim_unatt_test = music_stim_unatt[i*(round(music_sample_len/k_cv)):(i+1)*(round(music_sample_len/k_cv)),:]\n",
    "\n",
    "    speech_eeg_train = np.concatenate((speech_eeg[:i*(round(speech_sample_len/k_cv)),:],speech_eeg[(i+1)*(round(speech_sample_len/k_cv)):,:]),axis=0)\n",
    "    speech_stim_train = np.concatenate((speech_stim_att[:i*(round(speech_sample_len/k_cv)),:],speech_stim_att[(i+1)*(round(speech_sample_len/k_cv)):,:]),axis=0)\n",
    "\n",
    "    music_eeg_train = np.concatenate((music_eeg[:i*(round(music_sample_len/k_cv)),:],music_eeg[(i+1)*(round(music_sample_len/k_cv)):,:]),axis=0)\n",
    "    music_stim_train = np.concatenate((music_stim_att[:i*(round(music_sample_len/k_cv)),:],music_stim_att[(i+1)*(round(music_sample_len/k_cv)):,:]),axis=0)\n",
    "\n",
    "    eeg_train = music_eeg_train\n",
    "    stim_train = music_stim_train\n",
    "    \n",
    "    #Lags\n",
    "    \n",
    "    eeg_train = lag_generator_new(eeg_train,lags_neuro)\n",
    "    stim_train = lag_generator_new(stim_train,lags_stim)\n",
    "    \n",
    "    speech_eeg_test = lag_generator_new(speech_eeg_test,lags_neuro)\n",
    "    speech_stim_att_test = lag_generator_new(speech_stim_att_test,lags_stim)\n",
    "    speech_stim_unatt_test = lag_generator_new(speech_stim_unatt_test,lags_stim)\n",
    "\n",
    "    music_eeg_test = lag_generator_new(music_eeg_test,lags_neuro)\n",
    "    music_stim_att_test = lag_generator_new(music_stim_att_test,lags_stim)\n",
    "    music_stim_unatt_test = lag_generator_new(music_stim_unatt_test,lags_stim)\n",
    "\n",
    "    #Training\n",
    "    \n",
    "    cca_att = CCA(n_components=3)\n",
    "    cca_att = cca_att.fit(eeg_train, stim_train)\n",
    "\n",
    "    #Evaluations\n",
    "    \n",
    "    X_c, Y_c = cca_att.transform(eeg_train, stim_train)\n",
    "    r_fwd = pearsonr(np.squeeze(X_c.flatten()), np.squeeze(Y_c.flatten())).statistic\n",
    "    print(f\"Train: {r_fwd.round(3)}\")\n",
    "    train_corrs.append(r_fwd)\n",
    "\n",
    "    X_c, Y_c = cca_att.transform(speech_eeg_test, speech_stim_att_test)\n",
    "    r_fwd = pearsonr(np.squeeze(X_c.flatten()), np.squeeze(Y_c.flatten())).statistic\n",
    "    print(f\"Attended Speech: {r_fwd.round(3)}\")\n",
    "    speech_att_test_corrs.append(r_fwd)\n",
    "    \n",
    "    X_c, Y_c = cca_att.transform(music_eeg_test, music_stim_unatt_test)\n",
    "    r_fwd = pearsonr(np.squeeze(X_c.flatten()), np.squeeze(Y_c.flatten())).statistic\n",
    "    print(f\"Unattended Speech: {r_fwd.round(3)}\")\n",
    "    music_unatt_test_corrs.append(r_fwd)\n",
    "\n",
    "    X_c, Y_c = cca_att.transform(music_eeg_test, music_stim_att_test)\n",
    "    r_fwd = pearsonr(np.squeeze(X_c.flatten()), np.squeeze(Y_c.flatten())).statistic\n",
    "    print(f\"Attended Music: {r_fwd.round(3)}\")\n",
    "    music_att_test_corrs.append(r_fwd)\n",
    "\n",
    "    X_c, Y_c = cca_att.transform(speech_eeg_test, speech_stim_unatt_test)\n",
    "    r_fwd = pearsonr(np.squeeze(X_c.flatten()), np.squeeze(Y_c.flatten())).statistic\n",
    "    print(f\"Unattended Music: {r_fwd.round(3)}\")\n",
    "    speech_unatt_test_corrs.append(r_fwd)\n",
    "\n",
    "\n",
    "print(f'Average Training Correlation: {np.mean(train_corrs)}')\n",
    "print(f'Average Attended Speech Test Correlation: {np.mean(speech_att_test_corrs)}')\n",
    "print(f'Average Unttended Speech Test Correlation: {np.mean(music_unatt_test_corrs)}')\n",
    "print(f'Average Attended Music Test Correlation: {np.mean(music_att_test_corrs)}')\n",
    "print(f'Average Unttended Music Test Correlation: {np.mean(speech_unatt_test_corrs)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "31e03fd5-2510-421f-ada6-723d29f04c4d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Speech-Attended Music-Unattended AAD Accuracy: 0.5\n",
      "Music-Attended Speech-Unattended AAD Accuracy: 1.0\n"
     ]
    }
   ],
   "source": [
    "print(f\"Speech-Attended Music-Unattended AAD Accuracy: {np.mean([True if speech_att_test_corrs[i] > speech_unatt_test_corrs[i] else False for i in range(len(speech_att_test_corrs))])}\")\n",
    "print(f\"Music-Attended Speech-Unattended AAD Accuracy: {np.mean([True if music_att_test_corrs[i] > music_unatt_test_corrs[i] else False for i in range(len(music_att_test_corrs))])}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "77bc6530-3e8b-410a-8c79-46a21950ad43",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Attended Music Test Correlation: 0.061638973857888045\n",
      "Std Attended Music Test Correlation: 0.02302003738620022\n",
      "Average Unattended Music Test Correlation: 0.03399504068299472\n",
      "Std Unattended Music Test Correlation: 0.02348674120800301\n"
     ]
    }
   ],
   "source": [
    "print(f'Average Attended Music Test Correlation: {np.mean(music_att_test_corrs)}')\n",
    "print(f'Std Attended Music Test Correlation: {np.std(music_att_test_corrs)}')\n",
    "print(f'Average Unattended Music Test Correlation: {np.mean(speech_unatt_test_corrs)}')\n",
    "print(f'Std Unattended Music Test Correlation: {np.std(speech_unatt_test_corrs)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9e0499a0-7744-40de-9313-808b71495e5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # save to disk\n",
    "# np.save('Weights/CCA_Multi_Music_Train_Envelope_X_Weights.npy', cca_att.x_weights_)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (cu)",
   "language": "python",
   "name": "cu"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
