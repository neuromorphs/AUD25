{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f8dcdc3f-bf4f-4554-9a67-2d48262955e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import mtrf\n",
    "import librosa\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from IPython.display import Audio, display\n",
    "from sklearn.cross_decomposition import CCA\n",
    "\n",
    "from scipy.io import loadmat\n",
    "from scipy import linalg\n",
    "from scipy import stats\n",
    "from scipy.signal import hilbert, resample\n",
    "from scipy.stats import zscore, pearsonr\n",
    "\n",
    "from mtrf.model import TRF\n",
    "from sklearn.cross_decomposition import CCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "593b6220-8568-44af-91ca-5bc99f048e98",
   "metadata": {},
   "outputs": [],
   "source": [
    "def lag_generator_new(r, lags):\n",
    "    '''\n",
    "    Args:\n",
    "      r: [time, neurons]\n",
    "      \n",
    "    Return\n",
    "      out: [time, neuron*lags]\n",
    "    \n",
    "    '''\n",
    "    lags = list(range(lags[0], lags[1]+1))\n",
    "    out = np.zeros([r.shape[0], r.shape[1]*len(lags)])\n",
    "    r = np.pad(r, ((0,len(lags)),(0,0)), 'constant')\n",
    "\n",
    "    r_lag_list = []\n",
    "    \n",
    "    for lag in lags:\n",
    "        t1 = np.roll(r, lag, axis=0)\n",
    "        if lag < 0:\n",
    "            t1[lag-1:, :] = 0\n",
    "        else:\n",
    "            t1[:lag, :] = 0\n",
    "            \n",
    "        r_lag_list.append(t1[:out.shape[0], :])\n",
    "        \n",
    "    out = np.concatenate(r_lag_list, axis=1)\n",
    "    \n",
    "    return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "11f7a4bc-5ee0-4da0-9007-dd26a2d7031d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#trials = os.listdir('../../../Data/Cindy/Preprocessed/preprocessed_mixed_new')\n",
    "#trials.remove('cindy_mixed_pp_record.csv')\n",
    "\n",
    "folder_name = '../../../Data/Cindy/Preprocessed/preprocessed_mixed_01_15Hz'\n",
    "trials = os.listdir(folder_name)\n",
    "trials = [item for item in trials if item not in ['cindy_mixed_pp_record.csv','.ipynb_checkpoints.mat','.ipynb_checkpoints']]\n",
    "\n",
    "\n",
    "# folder_name = '../../../Data/Samet/Preprocessed/preprocessed_mixed_01_15Hz'\n",
    "# trials = os.listdir(folder_name)\n",
    "# trials = [item for item in trials if item not in ['multi1_pp_record.csv','multi2_pp_record.csv','multi3_pp_record.csv','multi4_pp_record.csv','.ipynb_checkpoints.mat','.ipynb_checkpoints']]\n",
    "\n",
    "fs_eeg = 128\n",
    "\n",
    "lags_neuro = [-40, 10]\n",
    "lags_stim = [-20, 10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "52b09784-ebeb-4cb1-978f-4d83ce2d32c2",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cindy_mixed_music_32.mat\n",
      "cindy_mixed_Music_26.mat\n",
      "cindy_mixed_Music_27.mat\n",
      "cindy_mixed_Music_33.mat\n",
      "cindy_mixed_music_25.mat\n",
      "cindy_mixed_Music_31.mat\n",
      "cindy_mixed_music_19.mat\n",
      "cindy_mixed_Music_18.mat\n",
      "cindy_mixed_Music_30.mat\n",
      "cindy_mixed_Music_24.mat\n",
      "cindy_mixed_Music_20.mat\n",
      "cindy_mixed_music_34.mat\n",
      "cindy_mixed_music_35.mat\n",
      "cindy_mixed_Music_21.mat\n",
      "cindy_mixed_music_37.mat\n",
      "cindy_mixed_music_23.mat\n",
      "cindy_mixed_music_22.mat\n",
      "cindy_mixed_Music_36.mat\n",
      "cindy_mixed_Music_8.mat\n",
      "cindy_mixed_Speech_1.mat\n",
      "cindy_mixed_Speech_17.mat\n",
      "cindy_mixed_speech_16.mat\n",
      "cindy_mixed_Speech_0.mat\n",
      "cindy_mixed_music_9.mat\n",
      "cindy_mixed_speech_2.mat\n",
      "cindy_mixed_Speech_14.mat\n",
      "cindy_mixed_Speech_28.mat\n",
      "cindy_mixed_Speech_29.mat\n",
      "cindy_mixed_Speech_15.mat\n",
      "cindy_mixed_Speech_3.mat\n",
      "cindy_mixed_Speech_7.mat\n",
      "cindy_mixed_speech_11.mat\n",
      "cindy_mixed_speech_10.mat\n",
      "cindy_mixed_Speech_6.mat\n",
      "cindy_mixed_speech_4.mat\n",
      "cindy_mixed_Speech_12.mat\n",
      "cindy_mixed_speech_13.mat\n",
      "cindy_mixed_speech_5.mat\n",
      "cindy_mixed_music_1.mat\n",
      "cindy_mixed_speech_8.mat\n",
      "cindy_mixed_speech_36.mat\n",
      "cindy_mixed_Speech_22.mat\n",
      "cindy_mixed_Speech_23.mat\n",
      "cindy_mixed_Speech_37.mat\n",
      "cindy_mixed_Speech_9.mat\n",
      "cindy_mixed_Music_0.mat\n",
      "cindy_mixed_Music_2.mat\n",
      "cindy_mixed_speech_21.mat\n",
      "cindy_mixed_Speech_35.mat\n",
      "cindy_mixed_Speech_34.mat\n",
      "cindy_mixed_speech_20.mat\n",
      "cindy_mixed_music_3.mat\n",
      "cindy_mixed_music_7.mat\n",
      "cindy_mixed_speech_18.mat\n",
      "cindy_mixed_speech_24.mat\n",
      "cindy_mixed_speech_30.mat\n",
      "cindy_mixed_speech_31.mat\n",
      "cindy_mixed_Speech_25.mat\n",
      "cindy_mixed_Speech_19.mat\n",
      "cindy_mixed_music_6.mat\n",
      "cindy_mixed_Music_4.mat\n",
      "cindy_mixed_speech_33.mat\n",
      "cindy_mixed_speech_27.mat\n",
      "cindy_mixed_speech_26.mat\n",
      "cindy_mixed_Speech_32.mat\n",
      "cindy_mixed_Music_5.mat\n",
      "cindy_mixed_Music_13.mat\n",
      "cindy_mixed_music_12.mat\n",
      "cindy_mixed_Music_10.mat\n",
      "cindy_mixed_Music_11.mat\n",
      "cindy_mixed_music_29.mat\n",
      "cindy_mixed_music_15.mat\n",
      "cindy_mixed_music_14.mat\n",
      "cindy_mixed_music_28.mat\n",
      "cindy_mixed_Music_16.mat\n",
      "cindy_mixed_music_17.mat\n"
     ]
    }
   ],
   "source": [
    "speech_eeg_all = []\n",
    "speech_att_env_all = []\n",
    "speech_unatt_env_all = []\n",
    "\n",
    "for trial in trials:\n",
    "    print(trial)\n",
    "    data = loadmat(os.path.join(folder_name,trial))\n",
    "    \n",
    "    if data['stim_attended'][0] == 'Speech':\n",
    "        att_stim,fs_audio = librosa.load(os.path.join(\"../../../Stimuli/Cindy/speech_only_short_22khz\",f\"{data['stimuli_speech'][0]}\"+'.wav'))\n",
    "        unatt_stim,fs_audio = librosa.load(os.path.join(\"../../../Stimuli/Cindy/piano_only_long_cropped_22khz\",f\"{data['stimuli_music'][0]}\"+'.wav'))\n",
    "    elif data['stim_attended'][0] == 'Music':\n",
    "        continue\n",
    "        att_stim,fs_audio = librosa.load(os.path.join(\"../../../Stimuli/Cindy/piano_only_long_cropped_22khz\",f\"{data['stimuli_music'][0]}\"+'.wav'))\n",
    "        unatt_stim,fs_audio = librosa.load(os.path.join(\"../../../Stimuli/Cindy/speech_only_short_22khz\",f\"{data['stimuli_speech'][0]}\"+'.wav'))\n",
    "        \n",
    "    \n",
    "    if data['stim_attended_pos'][0] == 'FirstHalfAttend':\n",
    "        att_stim = att_stim[:int(len(att_stim)/2)]\n",
    "        unatt_stim = unatt_stim[:int(len(unatt_stim)/2)]\n",
    "    elif data['stim_attended_pos'][0] == 'SecondHalfAttend':\n",
    "        att_stim = att_stim[int(len(att_stim)/2):]\n",
    "        unatt_stim = unatt_stim[int(len(unatt_stim)/2):]\n",
    "    \n",
    "    # display(Audio(att_stim,rate=fs_audio))\n",
    "    # display(Audio(unatt_stim,rate=fs_audio))\n",
    "    \n",
    "    att_env = np.abs(hilbert(zscore(att_stim)))\n",
    "    unatt_env = np.abs(hilbert(zscore(unatt_stim)))\n",
    "    \n",
    "    duration_sec = len(att_env) / fs_audio\n",
    "    n_target_samples = int(duration_sec * fs_eeg)\n",
    "    att_env = np.expand_dims(resample(att_env, n_target_samples),axis=0)\n",
    "    \n",
    "    duration_sec = len(unatt_env) / fs_audio\n",
    "    n_target_samples = int(duration_sec * fs_eeg)\n",
    "    unatt_env = np.expand_dims(resample(unatt_env, n_target_samples),axis=0)\n",
    "\n",
    "    speech_eeg_all.append(data['eeg_data'])\n",
    "    speech_att_env_all.append(att_env)\n",
    "    speech_unatt_env_all.append(unatt_env)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "945c4789-f5bb-4ada-827b-025c3a3b4195",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cindy_mixed_music_32.mat\n",
      "cindy_mixed_Music_26.mat\n",
      "cindy_mixed_Music_27.mat\n",
      "cindy_mixed_Music_33.mat\n",
      "cindy_mixed_music_25.mat\n",
      "cindy_mixed_Music_31.mat\n",
      "cindy_mixed_music_19.mat\n",
      "cindy_mixed_Music_18.mat\n",
      "cindy_mixed_Music_30.mat\n",
      "cindy_mixed_Music_24.mat\n",
      "cindy_mixed_Music_20.mat\n",
      "cindy_mixed_music_34.mat\n",
      "cindy_mixed_music_35.mat\n",
      "cindy_mixed_Music_21.mat\n",
      "cindy_mixed_music_37.mat\n",
      "cindy_mixed_music_23.mat\n",
      "cindy_mixed_music_22.mat\n",
      "cindy_mixed_Music_36.mat\n",
      "cindy_mixed_Music_8.mat\n",
      "cindy_mixed_Speech_1.mat\n",
      "cindy_mixed_Speech_17.mat\n",
      "cindy_mixed_speech_16.mat\n",
      "cindy_mixed_Speech_0.mat\n",
      "cindy_mixed_music_9.mat\n",
      "cindy_mixed_speech_2.mat\n",
      "cindy_mixed_Speech_14.mat\n",
      "cindy_mixed_Speech_28.mat\n",
      "cindy_mixed_Speech_29.mat\n",
      "cindy_mixed_Speech_15.mat\n",
      "cindy_mixed_Speech_3.mat\n",
      "cindy_mixed_Speech_7.mat\n",
      "cindy_mixed_speech_11.mat\n",
      "cindy_mixed_speech_10.mat\n",
      "cindy_mixed_Speech_6.mat\n",
      "cindy_mixed_speech_4.mat\n",
      "cindy_mixed_Speech_12.mat\n",
      "cindy_mixed_speech_13.mat\n",
      "cindy_mixed_speech_5.mat\n",
      "cindy_mixed_music_1.mat\n",
      "cindy_mixed_speech_8.mat\n",
      "cindy_mixed_speech_36.mat\n",
      "cindy_mixed_Speech_22.mat\n",
      "cindy_mixed_Speech_23.mat\n",
      "cindy_mixed_Speech_37.mat\n",
      "cindy_mixed_Speech_9.mat\n",
      "cindy_mixed_Music_0.mat\n",
      "cindy_mixed_Music_2.mat\n",
      "cindy_mixed_speech_21.mat\n",
      "cindy_mixed_Speech_35.mat\n",
      "cindy_mixed_Speech_34.mat\n",
      "cindy_mixed_speech_20.mat\n",
      "cindy_mixed_music_3.mat\n",
      "cindy_mixed_music_7.mat\n",
      "cindy_mixed_speech_18.mat\n",
      "cindy_mixed_speech_24.mat\n",
      "cindy_mixed_speech_30.mat\n",
      "cindy_mixed_speech_31.mat\n",
      "cindy_mixed_Speech_25.mat\n",
      "cindy_mixed_Speech_19.mat\n",
      "cindy_mixed_music_6.mat\n",
      "cindy_mixed_Music_4.mat\n",
      "cindy_mixed_speech_33.mat\n",
      "cindy_mixed_speech_27.mat\n",
      "cindy_mixed_speech_26.mat\n",
      "cindy_mixed_Speech_32.mat\n",
      "cindy_mixed_Music_5.mat\n",
      "cindy_mixed_Music_13.mat\n",
      "cindy_mixed_music_12.mat\n",
      "cindy_mixed_Music_10.mat\n",
      "cindy_mixed_Music_11.mat\n",
      "cindy_mixed_music_29.mat\n",
      "cindy_mixed_music_15.mat\n",
      "cindy_mixed_music_14.mat\n",
      "cindy_mixed_music_28.mat\n",
      "cindy_mixed_Music_16.mat\n",
      "cindy_mixed_music_17.mat\n"
     ]
    }
   ],
   "source": [
    "music_eeg_all = []\n",
    "music_att_env_all = []\n",
    "music_unatt_env_all = []\n",
    "\n",
    "for trial in trials:\n",
    "    print(trial)\n",
    "    data = loadmat(os.path.join(folder_name,trial))\n",
    "    \n",
    "    if data['stim_attended'][0] == 'Speech':\n",
    "        continue\n",
    "        att_stim,fs_audio = librosa.load(os.path.join(\"../../../Stimuli/Cindy/speech_only_short_22khz\",f\"{data['stimuli_speech'][0]}\"+'.wav'))\n",
    "        unatt_stim,fs_audio = librosa.load(os.path.join(\"../../../Stimuli/Cindy/piano_only_long_cropped_22khz\",f\"{data['stimuli_music'][0]}\"+'.wav'))\n",
    "    elif data['stim_attended'][0] == 'Music':\n",
    "        att_stim,fs_audio = librosa.load(os.path.join(\"../../../Stimuli/Cindy/piano_only_long_cropped_22khz\",f\"{data['stimuli_music'][0]}\"+'.wav'))\n",
    "        unatt_stim,fs_audio = librosa.load(os.path.join(\"../../../Stimuli/Cindy/speech_only_short_22khz\",f\"{data['stimuli_speech'][0]}\"+'.wav'))\n",
    "        \n",
    "    \n",
    "    if data['stim_attended_pos'][0] == 'FirstHalfAttend':\n",
    "        att_stim = att_stim[:int(len(att_stim)/2)]\n",
    "        unatt_stim = unatt_stim[:int(len(unatt_stim)/2)]\n",
    "    elif data['stim_attended_pos'][0] == 'SecondHalfAttend':\n",
    "        att_stim = att_stim[int(len(att_stim)/2):]\n",
    "        unatt_stim = unatt_stim[int(len(unatt_stim)/2):]\n",
    "    \n",
    "    # display(Audio(att_stim,rate=fs_audio))\n",
    "    # display(Audio(unatt_stim,rate=fs_audio))\n",
    "    \n",
    "    att_env = np.abs(hilbert(zscore(att_stim)))\n",
    "    unatt_env = np.abs(hilbert(zscore(unatt_stim)))\n",
    "    \n",
    "    duration_sec = len(att_env) / fs_audio\n",
    "    n_target_samples = int(duration_sec * fs_eeg)\n",
    "    att_env = np.expand_dims(resample(att_env, n_target_samples),axis=0)\n",
    "    \n",
    "    duration_sec = len(unatt_env) / fs_audio\n",
    "    n_target_samples = int(duration_sec * fs_eeg)\n",
    "    unatt_env = np.expand_dims(resample(unatt_env, n_target_samples),axis=0)\n",
    "\n",
    "    music_eeg_all.append(data['eeg_data'])\n",
    "    music_att_env_all.append(att_env)\n",
    "    music_unatt_env_all.append(unatt_env)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5c010a40-7c8f-496f-9f68-cb655fdc28ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# speech_eeg = np.concatenate(speech_eeg_all,axis=1).T\n",
    "# speech_stim_att = np.concatenate(speech_att_env_all,axis=1).T\n",
    "# speech_stim_unatt = np.concatenate(speech_unatt_env_all,axis=1).T\n",
    "# speech_eeg = zscore(speech_eeg, axis=0)\n",
    "# speech_stim_att = zscore(speech_stim_att, axis=0)\n",
    "# speech_stim_unatt = zscore(speech_stim_unatt, axis=0)\n",
    "\n",
    "# music_eeg = np.concatenate(music_eeg_all,axis=1).T\n",
    "# music_stim_att = np.concatenate(music_att_env_all,axis=1).T\n",
    "# music_stim_unatt = np.concatenate(music_unatt_env_all,axis=1).T\n",
    "# music_eeg = zscore(music_eeg, axis=0)\n",
    "# music_stim_att = zscore(music_stim_att, axis=0)\n",
    "# music_stim_unatt = zscore(music_stim_unatt, axis=0)\n",
    "\n",
    "speech_eeg = np.concatenate(speech_eeg_all,axis=1).T\n",
    "speech_stim_att = np.concatenate(speech_att_env_all,axis=1).T\n",
    "speech_stim_unatt = np.concatenate(speech_unatt_env_all,axis=1).T\n",
    "\n",
    "# speech_eeg_mean = (np.mean(speech_eeg,axis=0))\n",
    "# speech_eeg_std = (np.std(speech_eeg,axis=0))\n",
    "# speech_stim_att_mean = (np.mean(speech_stim_att,axis=0))\n",
    "# speech_stim_att_std = (np.std(speech_stim_att,axis=0))\n",
    "# speech_stim_unatt_mean = (np.mean(speech_stim_unatt,axis=0))\n",
    "# speech_stim_unatt_std = (np.std(speech_stim_unatt,axis=0))\n",
    "\n",
    "music_eeg = np.concatenate(music_eeg_all,axis=1).T\n",
    "music_stim_att = np.concatenate(music_att_env_all,axis=1).T\n",
    "music_stim_unatt = np.concatenate(music_unatt_env_all,axis=1).T\n",
    "\n",
    "# music_eeg_mean = (np.mean(music_eeg,axis=0))\n",
    "# music_eeg_std = (np.std(music_eeg,axis=0))\n",
    "# music_stim_att_mean = (np.mean(music_stim_att,axis=0))\n",
    "# music_stim_att_std = (np.std(music_stim_att,axis=0))\n",
    "# music_stim_unatt_mean = (np.mean(music_stim_unatt,axis=0))\n",
    "# music_stim_unatt_std = (np.std(music_stim_unatt,axis=0))\n",
    "\n",
    "eeg_mean = np.mean(np.concatenate((speech_eeg,music_eeg),axis = 0),axis=0)\n",
    "eeg_std = np.std(np.concatenate((speech_eeg,music_eeg),axis = 0),axis=0)\n",
    "\n",
    "stim_mean = np.mean(np.concatenate((speech_stim_att,speech_stim_unatt,music_stim_att,music_stim_unatt),axis = 0),axis=0)\n",
    "stim_std = np.std(np.concatenate((speech_stim_att,speech_stim_unatt,music_stim_att,music_stim_unatt),axis = 0),axis=0)\n",
    "\n",
    "speech_eeg = (speech_eeg-eeg_mean)/eeg_std\n",
    "music_eeg = (music_eeg-eeg_mean)/eeg_std\n",
    "\n",
    "speech_stim_att = (speech_stim_att-stim_mean)/stim_std\n",
    "speech_stim_unatt = (speech_stim_unatt-stim_mean)/stim_std\n",
    "music_stim_att = (music_stim_att-stim_mean)/stim_std\n",
    "music_stim_unatt = (music_stim_unatt-stim_mean)/stim_std\n",
    "\n",
    "\n",
    "# speech_eeg = zscore(speech_eeg, axis=0)\n",
    "# speech_stim_att = zscore(speech_stim_att, axis=0)\n",
    "# speech_stim_unatt = zscore(speech_stim_unatt, axis=0)\n",
    "\n",
    "# music_eeg = zscore(music_eeg, axis=0)\n",
    "# music_stim_att = zscore(music_stim_att, axis=0)\n",
    "# music_stim_unatt = zscore(music_stim_unatt, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a5d274ec-1266-493b-999d-9305854a7530",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Split 1\n",
      "Train: 0.119\n",
      "Attended Speech: 0.031\n",
      "Unattended Speech: 0.041\n",
      "Attended Music: -0.053\n",
      "Unattended Music: -0.05\n",
      "Split 2\n",
      "Train: 0.123\n",
      "Attended Speech: -0.016\n",
      "Unattended Speech: 0.002\n",
      "Attended Music: -0.023\n",
      "Unattended Music: 0.078\n",
      "Split 3\n",
      "Train: 0.108\n",
      "Attended Speech: 0.013\n",
      "Unattended Speech: 0.038\n",
      "Attended Music: 0.002\n",
      "Unattended Music: -0.0\n",
      "Split 4\n",
      "Train: 0.117\n",
      "Attended Speech: 0.046\n",
      "Unattended Speech: 0.055\n",
      "Attended Music: -0.135\n",
      "Unattended Music: -0.008\n",
      "Split 5\n",
      "Train: 0.116\n",
      "Attended Speech: 0.124\n",
      "Unattended Speech: 0.105\n",
      "Attended Music: -0.035\n",
      "Unattended Music: -0.032\n",
      "Split 6\n",
      "Train: 0.114\n",
      "Attended Speech: 0.086\n",
      "Unattended Speech: 0.025\n",
      "Attended Music: 0.096\n",
      "Unattended Music: 0.053\n",
      "Split 7\n",
      "Train: 0.108\n",
      "Attended Speech: 0.046\n",
      "Unattended Speech: 0.001\n",
      "Attended Music: 0.037\n",
      "Unattended Music: 0.022\n",
      "Split 8\n",
      "Train: 0.118\n",
      "Attended Speech: 0.049\n",
      "Unattended Speech: 0.039\n",
      "Attended Music: -0.04\n",
      "Unattended Music: 0.065\n",
      "Split 9\n",
      "Train: 0.117\n",
      "Attended Speech: 0.014\n",
      "Unattended Speech: 0.121\n",
      "Attended Music: 0.015\n",
      "Unattended Music: 0.028\n",
      "Split 10\n",
      "Train: 0.114\n",
      "Attended Speech: 0.125\n",
      "Unattended Speech: 0.024\n",
      "Attended Music: -0.079\n",
      "Unattended Music: -0.019\n",
      "Split 11\n",
      "Train: 0.119\n",
      "Attended Speech: 0.09\n",
      "Unattended Speech: 0.078\n",
      "Attended Music: -0.061\n",
      "Unattended Music: -0.021\n",
      "Split 12\n",
      "Train: 0.117\n",
      "Attended Speech: 0.131\n",
      "Unattended Speech: -0.091\n",
      "Attended Music: -0.101\n",
      "Unattended Music: -0.025\n",
      "Split 13\n",
      "Train: 0.113\n",
      "Attended Speech: 0.056\n",
      "Unattended Speech: 0.027\n",
      "Attended Music: -0.053\n",
      "Unattended Music: 0.037\n",
      "Split 14\n",
      "Train: 0.114\n",
      "Attended Speech: 0.079\n",
      "Unattended Speech: 0.084\n",
      "Attended Music: -0.048\n",
      "Unattended Music: -0.024\n",
      "Split 15\n",
      "Train: 0.114\n",
      "Attended Speech: 0.019\n",
      "Unattended Speech: 0.024\n",
      "Attended Music: -0.008\n",
      "Unattended Music: 0.02\n",
      "Split 16\n",
      "Train: 0.118\n",
      "Attended Speech: 0.054\n",
      "Unattended Speech: 0.099\n",
      "Attended Music: 0.057\n",
      "Unattended Music: -0.055\n",
      "Split 17\n",
      "Train: 0.119\n",
      "Attended Speech: 0.025\n",
      "Unattended Speech: -0.012\n",
      "Attended Music: 0.038\n",
      "Unattended Music: -0.008\n",
      "Split 18\n",
      "Train: 0.116\n",
      "Attended Speech: 0.017\n",
      "Unattended Speech: 0.003\n",
      "Attended Music: 0.001\n",
      "Unattended Music: 0.09\n",
      "Split 19\n",
      "Train: 0.115\n",
      "Attended Speech: 0.043\n",
      "Unattended Speech: 0.023\n",
      "Attended Music: 0.007\n",
      "Unattended Music: 0.046\n",
      "Split 20\n",
      "Train: 0.113\n",
      "Attended Speech: 0.081\n",
      "Unattended Speech: 0.069\n",
      "Attended Music: -0.018\n",
      "Unattended Music: 0.018\n",
      "Average Training Correlation: 0.11554706476039385\n",
      "Average Attended Speech Test Correlation: 0.055623742574860656\n",
      "Average Unttended Speech Test Correlation: 0.03784230123293935\n",
      "Average Attended Music Test Correlation: -0.020077442387467427\n",
      "Average Unttended Music Test Correlation: 0.010732613759806614\n"
     ]
    }
   ],
   "source": [
    "train_corrs = []\n",
    "speech_att_test_corrs = []\n",
    "speech_unatt_test_corrs = []\n",
    "music_att_test_corrs = []\n",
    "music_unatt_test_corrs = []\n",
    "\n",
    "\n",
    "speech_sample_len = speech_eeg.shape[0]\n",
    "music_sample_len = music_eeg.shape[0]\n",
    "\n",
    "k_cv = 20\n",
    "for i in range(k_cv):\n",
    "    print(f'Split {i+1}')\n",
    "\n",
    "    #Train Test Split\n",
    "    \n",
    "    speech_eeg_test = speech_eeg[i*(round(speech_sample_len/k_cv)):(i+1)*(round(speech_sample_len/k_cv)),:]\n",
    "    speech_stim_att_test = speech_stim_att[i*(round(speech_sample_len/k_cv)):(i+1)*(round(speech_sample_len/k_cv)),:]\n",
    "    speech_stim_unatt_test = speech_stim_unatt[i*(round(speech_sample_len/k_cv)):(i+1)*(round(speech_sample_len/k_cv)),:]\n",
    "\n",
    "    music_eeg_test = music_eeg[i*(round(music_sample_len/k_cv)):(i+1)*(round(music_sample_len/k_cv)),:]\n",
    "    music_stim_att_test = music_stim_att[i*(round(music_sample_len/k_cv)):(i+1)*(round(music_sample_len/k_cv)),:]\n",
    "    music_stim_unatt_test = music_stim_unatt[i*(round(music_sample_len/k_cv)):(i+1)*(round(music_sample_len/k_cv)),:]\n",
    "\n",
    "    speech_eeg_train = np.concatenate((speech_eeg[:i*(round(speech_sample_len/k_cv)),:],speech_eeg[(i+1)*(round(speech_sample_len/k_cv)):,:]),axis=0)\n",
    "    speech_stim_train = np.concatenate((speech_stim_att[:i*(round(speech_sample_len/k_cv)),:],speech_stim_att[(i+1)*(round(speech_sample_len/k_cv)):,:]),axis=0)\n",
    "\n",
    "    music_eeg_train = np.concatenate((music_eeg[:i*(round(music_sample_len/k_cv)),:],music_eeg[(i+1)*(round(music_sample_len/k_cv)):,:]),axis=0)\n",
    "    music_stim_train = np.concatenate((music_stim_att[:i*(round(music_sample_len/k_cv)),:],music_stim_att[(i+1)*(round(music_sample_len/k_cv)):,:]),axis=0)\n",
    "\n",
    "    eeg_train = np.concatenate((speech_eeg_train,music_eeg_train),axis=0)\n",
    "    stim_train = np.concatenate((speech_stim_train,music_stim_train),axis=0)\n",
    "\n",
    "    #Lags\n",
    "    \n",
    "    eeg_train = lag_generator_new(eeg_train,lags_neuro)\n",
    "    stim_train = lag_generator_new(stim_train,lags_stim)\n",
    "    \n",
    "    speech_eeg_test = lag_generator_new(speech_eeg_test,lags_neuro)\n",
    "    speech_stim_att_test = lag_generator_new(speech_stim_att_test,lags_stim)\n",
    "    speech_stim_unatt_test = lag_generator_new(speech_stim_unatt_test,lags_stim)\n",
    "\n",
    "    music_eeg_test = lag_generator_new(music_eeg_test,lags_neuro)\n",
    "    music_stim_att_test = lag_generator_new(music_stim_att_test,lags_stim)\n",
    "    music_stim_unatt_test = lag_generator_new(music_stim_unatt_test,lags_stim)\n",
    "\n",
    "    #Training\n",
    "    \n",
    "    cca_att = CCA(n_components=5)\n",
    "    cca_att = cca_att.fit(eeg_train, stim_train)\n",
    "\n",
    "    #Evaluations\n",
    "    \n",
    "    X_c, Y_c = cca_att.transform(eeg_train, stim_train)\n",
    "    r_fwd = pearsonr(np.squeeze(X_c.flatten()), np.squeeze(Y_c.flatten())).statistic\n",
    "    print(f\"Train: {r_fwd.round(3)}\")\n",
    "    train_corrs.append(r_fwd)\n",
    "\n",
    "    X_c, Y_c = cca_att.transform(speech_eeg_test, speech_stim_att_test)\n",
    "    r_fwd = pearsonr(np.squeeze(X_c.flatten()), np.squeeze(Y_c.flatten())).statistic\n",
    "    print(f\"Attended Speech: {r_fwd.round(3)}\")\n",
    "    speech_att_test_corrs.append(r_fwd)\n",
    "    \n",
    "    X_c, Y_c = cca_att.transform(music_eeg_test, music_stim_unatt_test)\n",
    "    r_fwd = pearsonr(np.squeeze(X_c.flatten()), np.squeeze(Y_c.flatten())).statistic\n",
    "    print(f\"Unattended Speech: {r_fwd.round(3)}\")\n",
    "    music_unatt_test_corrs.append(r_fwd)\n",
    "\n",
    "    X_c, Y_c = cca_att.transform(music_eeg_test, music_stim_att_test)\n",
    "    r_fwd = pearsonr(np.squeeze(X_c.flatten()), np.squeeze(Y_c.flatten())).statistic\n",
    "    print(f\"Attended Music: {r_fwd.round(3)}\")\n",
    "    music_att_test_corrs.append(r_fwd)\n",
    "\n",
    "    X_c, Y_c = cca_att.transform(speech_eeg_test, speech_stim_unatt_test)\n",
    "    r_fwd = pearsonr(np.squeeze(X_c.flatten()), np.squeeze(Y_c.flatten())).statistic\n",
    "    print(f\"Unattended Music: {r_fwd.round(3)}\")\n",
    "    speech_unatt_test_corrs.append(r_fwd)\n",
    "\n",
    "\n",
    "print(f'Average Training Correlation: {np.mean(train_corrs)}')\n",
    "print(f'Average Attended Speech Test Correlation: {np.mean(speech_att_test_corrs)}')\n",
    "print(f'Average Unttended Speech Test Correlation: {np.mean(music_unatt_test_corrs)}')\n",
    "print(f'Average Attended Music Test Correlation: {np.mean(music_att_test_corrs)}')\n",
    "print(f'Average Unttended Music Test Correlation: {np.mean(speech_unatt_test_corrs)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "31e03fd5-2510-421f-ada6-723d29f04c4d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Speech-Attended Music-Unattended AAD Accuracy: 0.7\n",
      "Music-Attended Speech-Unattended AAD Accuracy: 0.15\n"
     ]
    }
   ],
   "source": [
    "print(f\"Speech-Attended Music-Unattended AAD Accuracy: {np.mean([True if speech_att_test_corrs[i] > speech_unatt_test_corrs[i] else False for i in range(len(speech_att_test_corrs))])}\")\n",
    "print(f\"Music-Attended Speech-Unattended AAD Accuracy: {np.mean([True if music_att_test_corrs[i] > music_unatt_test_corrs[i] else False for i in range(len(music_att_test_corrs))])}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "de4ff985-dcb8-4d3d-a076-43ab6639fe50",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # save to disk\n",
    "# np.save('Weights/CCA_Multi_All_Train_Envelope_X_Weights.npy', cca_att.x_weights_)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (cu)",
   "language": "python",
   "name": "cu"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
