{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f8dcdc3f-bf4f-4554-9a67-2d48262955e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import mtrf\n",
    "import librosa\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from IPython.display import Audio, display\n",
    "from sklearn.cross_decomposition import CCA\n",
    "\n",
    "from scipy.io import loadmat\n",
    "from scipy import linalg\n",
    "from scipy import stats\n",
    "from scipy.signal import hilbert, resample\n",
    "from scipy.stats import zscore, pearsonr\n",
    "\n",
    "from mtrf.model import TRF\n",
    "from sklearn.cross_decomposition import CCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "593b6220-8568-44af-91ca-5bc99f048e98",
   "metadata": {},
   "outputs": [],
   "source": [
    "def lag_generator_new(r, lags):\n",
    "    '''\n",
    "    Args:\n",
    "      r: [time, neurons]\n",
    "      \n",
    "    Return\n",
    "      out: [time, neuron*lags]\n",
    "    \n",
    "    '''\n",
    "    lags = list(range(lags[0], lags[1]+1))\n",
    "    out = np.zeros([r.shape[0], r.shape[1]*len(lags)])\n",
    "    r = np.pad(r, ((0,len(lags)),(0,0)), 'constant')\n",
    "\n",
    "    r_lag_list = []\n",
    "    \n",
    "    for lag in lags:\n",
    "        t1 = np.roll(r, lag, axis=0)\n",
    "        if lag < 0:\n",
    "            t1[lag-1:, :] = 0\n",
    "        else:\n",
    "            t1[:lag, :] = 0\n",
    "            \n",
    "        r_lag_list.append(t1[:out.shape[0], :])\n",
    "        \n",
    "    out = np.concatenate(r_lag_list, axis=1)\n",
    "    \n",
    "    return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "11f7a4bc-5ee0-4da0-9007-dd26a2d7031d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#trials = os.listdir('../../../Data/Cindy/Preprocessed/preprocessed_mixed_new')\n",
    "#trials.remove('cindy_mixed_pp_record.csv')\n",
    "\n",
    "folder_name = '../../../Data/Samet/Preprocessed/preprocessed_mixed_01_15Hz'\n",
    "trials = os.listdir(folder_name)\n",
    "trials = [item for item in trials if item not in ['multi1_pp_record.csv','multi2_pp_record.csv','multi3_pp_record.csv','multi4_pp_record.csv','.ipynb_checkpoints.mat','.ipynb_checkpoints']]\n",
    "\n",
    "fs_eeg = 128\n",
    "\n",
    "lags_neuro = [-40, 10]\n",
    "lags_stim = [-10, 10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "52b09784-ebeb-4cb1-978f-4d83ce2d32c2",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "samet_Music_4.mat\n",
      "samet_Music_27.mat\n",
      "samet_Music_33.mat\n",
      "samet_Music_32.mat\n",
      "samet_Music_26.mat\n",
      "samet_Music_5.mat\n",
      "samet_Music_7.mat\n",
      "samet_Music_18.mat\n",
      "samet_Music_30.mat\n",
      "samet_Music_24.mat\n",
      "samet_Music_25.mat\n",
      "samet_Music_31.mat\n",
      "samet_Music_19.mat\n",
      "samet_Music_6.mat\n",
      "samet_Music_2.mat\n",
      "samet_Music_35.mat\n",
      "samet_Music_21.mat\n",
      "samet_Music_20.mat\n",
      "samet_Music_34.mat\n",
      "samet_Music_3.mat\n",
      "samet_Music_1.mat\n",
      "samet_Music_22.mat\n",
      "samet_Music_36.mat\n",
      "samet_Music_37.mat\n",
      "samet_Music_23.mat\n",
      "samet_Music_0.mat\n",
      "samet_Speech_34.mat\n",
      "samet_Speech_20.mat\n",
      "samet_Speech_0.mat\n",
      "samet_Speech_1.mat\n",
      "samet_Speech_21.mat\n",
      "samet_Speech_35.mat\n",
      "samet_Speech_23.mat\n",
      "samet_Speech_37.mat\n",
      "samet_Speech_3.mat\n",
      "samet_Speech_2.mat\n",
      "samet_Speech_36.mat\n",
      "samet_Speech_22.mat\n",
      "samet_Speech_26.mat\n",
      "samet_Speech_32.mat\n",
      "samet_Speech_6.mat\n",
      "samet_Speech_7.mat\n",
      "samet_Speech_33.mat\n",
      "samet_Speech_27.mat\n",
      "samet_Speech_31.mat\n",
      "samet_Speech_25.mat\n",
      "samet_Speech_19.mat\n",
      "samet_Speech_5.mat\n",
      "samet_Speech_4.mat\n",
      "samet_Speech_18.mat\n",
      "samet_Speech_24.mat\n",
      "samet_Speech_30.mat\n",
      "samet_Speech_29.mat\n",
      "samet_Speech_15.mat\n",
      "samet_Speech_9.mat\n",
      "samet_Speech_8.mat\n",
      "samet_Speech_14.mat\n",
      "samet_Speech_28.mat\n",
      "samet_Speech_16.mat\n",
      "samet_Speech_17.mat\n",
      "samet_Speech_13.mat\n",
      "samet_Speech_12.mat\n",
      "samet_Speech_10.mat\n",
      "samet_Speech_11.mat\n",
      "samet_Music_12.mat\n",
      "samet_Music_13.mat\n",
      "samet_Music_11.mat\n",
      "samet_Music_10.mat\n",
      "samet_Music_14.mat\n",
      "samet_Music_28.mat\n",
      "samet_Music_29.mat\n",
      "samet_Music_15.mat\n",
      "samet_Music_8.mat\n",
      "samet_Music_17.mat\n",
      "samet_Music_16.mat\n",
      "samet_Music_9.mat\n"
     ]
    }
   ],
   "source": [
    "speech_eeg_all = []\n",
    "speech_att_env_all = []\n",
    "speech_unatt_env_all = []\n",
    "\n",
    "for trial in trials:\n",
    "    print(trial)\n",
    "    data = loadmat(os.path.join(folder_name,trial))\n",
    "    \n",
    "    if data['stim_attended'][0] == 'Speech':\n",
    "        att_stim,fs_audio = librosa.load(os.path.join(\"../../../Stimuli/Cindy/speech_only_short_22khz\",f\"{data['stimuli_speech'][0]}\"+'.wav'))\n",
    "        unatt_stim,fs_audio = librosa.load(os.path.join(\"../../../Stimuli/Cindy/piano_only_long_cropped_22khz\",f\"{data['stimuli_music'][0]}\"+'.wav'))\n",
    "    elif data['stim_attended'][0] == 'Music':\n",
    "        continue\n",
    "        att_stim,fs_audio = librosa.load(os.path.join(\"../../../Stimuli/Cindy/piano_only_long_cropped_22khz\",f\"{data['stimuli_music'][0]}\"+'.wav'))\n",
    "        unatt_stim,fs_audio = librosa.load(os.path.join(\"../../../Stimuli/Cindy/speech_only_short_22khz\",f\"{data['stimuli_speech'][0]}\"+'.wav'))\n",
    "        \n",
    "    \n",
    "    if data['stim_attended_pos'][0] == 'FirstHalfAttend':\n",
    "        att_stim = att_stim[:int(len(att_stim)/2)]\n",
    "        unatt_stim = unatt_stim[:int(len(unatt_stim)/2)]\n",
    "    elif data['stim_attended_pos'][0] == 'SecondHalfAttend':\n",
    "        att_stim = att_stim[int(len(att_stim)/2):]\n",
    "        unatt_stim = unatt_stim[int(len(unatt_stim)/2):]\n",
    "    \n",
    "    # display(Audio(att_stim,rate=fs_audio))\n",
    "    # display(Audio(unatt_stim,rate=fs_audio))\n",
    "    \n",
    "    att_env = np.abs(hilbert(att_stim))\n",
    "    unatt_env = np.abs(hilbert(unatt_stim))\n",
    "    \n",
    "    duration_sec = len(att_env) / fs_audio\n",
    "    n_target_samples = int(duration_sec * fs_eeg)\n",
    "    att_env = np.expand_dims(resample(att_env, n_target_samples),axis=0)\n",
    "    \n",
    "    duration_sec = len(unatt_env) / fs_audio\n",
    "    n_target_samples = int(duration_sec * fs_eeg)\n",
    "    unatt_env = np.expand_dims(resample(unatt_env, n_target_samples),axis=0)\n",
    "\n",
    "    speech_eeg_all.append(data['eeg_data'])\n",
    "    speech_att_env_all.append(att_env)\n",
    "    speech_unatt_env_all.append(unatt_env)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "945c4789-f5bb-4ada-827b-025c3a3b4195",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "samet_Music_4.mat\n",
      "samet_Music_27.mat\n",
      "samet_Music_33.mat\n",
      "samet_Music_32.mat\n",
      "samet_Music_26.mat\n",
      "samet_Music_5.mat\n",
      "samet_Music_7.mat\n",
      "samet_Music_18.mat\n",
      "samet_Music_30.mat\n",
      "samet_Music_24.mat\n",
      "samet_Music_25.mat\n",
      "samet_Music_31.mat\n",
      "samet_Music_19.mat\n",
      "samet_Music_6.mat\n",
      "samet_Music_2.mat\n",
      "samet_Music_35.mat\n",
      "samet_Music_21.mat\n",
      "samet_Music_20.mat\n",
      "samet_Music_34.mat\n",
      "samet_Music_3.mat\n",
      "samet_Music_1.mat\n",
      "samet_Music_22.mat\n",
      "samet_Music_36.mat\n",
      "samet_Music_37.mat\n",
      "samet_Music_23.mat\n",
      "samet_Music_0.mat\n",
      "samet_Speech_34.mat\n",
      "samet_Speech_20.mat\n",
      "samet_Speech_0.mat\n",
      "samet_Speech_1.mat\n",
      "samet_Speech_21.mat\n",
      "samet_Speech_35.mat\n",
      "samet_Speech_23.mat\n",
      "samet_Speech_37.mat\n",
      "samet_Speech_3.mat\n",
      "samet_Speech_2.mat\n",
      "samet_Speech_36.mat\n",
      "samet_Speech_22.mat\n",
      "samet_Speech_26.mat\n",
      "samet_Speech_32.mat\n",
      "samet_Speech_6.mat\n",
      "samet_Speech_7.mat\n",
      "samet_Speech_33.mat\n",
      "samet_Speech_27.mat\n",
      "samet_Speech_31.mat\n",
      "samet_Speech_25.mat\n",
      "samet_Speech_19.mat\n",
      "samet_Speech_5.mat\n",
      "samet_Speech_4.mat\n",
      "samet_Speech_18.mat\n",
      "samet_Speech_24.mat\n",
      "samet_Speech_30.mat\n",
      "samet_Speech_29.mat\n",
      "samet_Speech_15.mat\n",
      "samet_Speech_9.mat\n",
      "samet_Speech_8.mat\n",
      "samet_Speech_14.mat\n",
      "samet_Speech_28.mat\n",
      "samet_Speech_16.mat\n",
      "samet_Speech_17.mat\n",
      "samet_Speech_13.mat\n",
      "samet_Speech_12.mat\n",
      "samet_Speech_10.mat\n",
      "samet_Speech_11.mat\n",
      "samet_Music_12.mat\n",
      "samet_Music_13.mat\n",
      "samet_Music_11.mat\n",
      "samet_Music_10.mat\n",
      "samet_Music_14.mat\n",
      "samet_Music_28.mat\n",
      "samet_Music_29.mat\n",
      "samet_Music_15.mat\n",
      "samet_Music_8.mat\n",
      "samet_Music_17.mat\n",
      "samet_Music_16.mat\n",
      "samet_Music_9.mat\n"
     ]
    }
   ],
   "source": [
    "music_eeg_all = []\n",
    "music_att_env_all = []\n",
    "music_unatt_env_all = []\n",
    "\n",
    "for trial in trials:\n",
    "    print(trial)\n",
    "    data = loadmat(os.path.join(folder_name,trial))\n",
    "    \n",
    "    if data['stim_attended'][0] == 'Speech':\n",
    "        continue\n",
    "        att_stim,fs_audio = librosa.load(os.path.join(\"../../../Stimuli/Cindy/speech_only_short_22khz\",f\"{data['stimuli_speech'][0]}\"+'.wav'))\n",
    "        unatt_stim,fs_audio = librosa.load(os.path.join(\"../../../Stimuli/Cindy/piano_only_long_cropped_22khz\",f\"{data['stimuli_music'][0]}\"+'.wav'))\n",
    "    elif data['stim_attended'][0] == 'Music':\n",
    "        att_stim,fs_audio = librosa.load(os.path.join(\"../../../Stimuli/Cindy/piano_only_long_cropped_22khz\",f\"{data['stimuli_music'][0]}\"+'.wav'))\n",
    "        unatt_stim,fs_audio = librosa.load(os.path.join(\"../../../Stimuli/Cindy/speech_only_short_22khz\",f\"{data['stimuli_speech'][0]}\"+'.wav'))\n",
    "        \n",
    "    \n",
    "    if data['stim_attended_pos'][0] == 'FirstHalfAttend':\n",
    "        att_stim = att_stim[:int(len(att_stim)/2)]\n",
    "        unatt_stim = unatt_stim[:int(len(unatt_stim)/2)]\n",
    "    elif data['stim_attended_pos'][0] == 'SecondHalfAttend':\n",
    "        att_stim = att_stim[int(len(att_stim)/2):]\n",
    "        unatt_stim = unatt_stim[int(len(unatt_stim)/2):]\n",
    "    \n",
    "    # display(Audio(att_stim,rate=fs_audio))\n",
    "    # display(Audio(unatt_stim,rate=fs_audio))\n",
    "    \n",
    "    att_env = np.abs(hilbert(att_stim))\n",
    "    unatt_env = np.abs(hilbert(unatt_stim))\n",
    "    \n",
    "    duration_sec = len(att_env) / fs_audio\n",
    "    n_target_samples = int(duration_sec * fs_eeg)\n",
    "    att_env = np.expand_dims(resample(att_env, n_target_samples),axis=0)\n",
    "    \n",
    "    duration_sec = len(unatt_env) / fs_audio\n",
    "    n_target_samples = int(duration_sec * fs_eeg)\n",
    "    unatt_env = np.expand_dims(resample(unatt_env, n_target_samples),axis=0)\n",
    "\n",
    "    music_eeg_all.append(data['eeg_data'])\n",
    "    music_att_env_all.append(att_env)\n",
    "    music_unatt_env_all.append(unatt_env)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5c010a40-7c8f-496f-9f68-cb655fdc28ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "speech_eeg = np.concatenate(speech_eeg_all,axis=1).T\n",
    "speech_stim_att = np.concatenate(speech_att_env_all,axis=1).T\n",
    "speech_stim_unatt = np.concatenate(speech_unatt_env_all,axis=1).T\n",
    "speech_eeg = zscore(speech_eeg, axis=0)\n",
    "speech_stim_att = zscore(speech_stim_att, axis=0)\n",
    "speech_stim_unatt = zscore(speech_stim_unatt, axis=0)\n",
    "\n",
    "music_eeg = np.concatenate(music_eeg_all,axis=1).T\n",
    "music_stim_att = np.concatenate(music_att_env_all,axis=1).T\n",
    "music_stim_unatt = np.concatenate(music_unatt_env_all,axis=1).T\n",
    "music_eeg = zscore(music_eeg, axis=0)\n",
    "music_stim_att = zscore(music_stim_att, axis=0)\n",
    "music_stim_unatt = zscore(music_stim_unatt, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a5d274ec-1266-493b-999d-9305854a7530",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Split 1\n",
      "Speech-Trained Model\n",
      "Train: 0.196\n",
      "Attended Speech: 0.036\n",
      "Unattended Speech: 0.111\n",
      "Attended Music: 0.108\n",
      "Unattended Music: 0.004\n",
      "Music-Trained Model\n",
      "Train: 0.14\n",
      "Attended Speech: 0.045\n",
      "Unattended Speech: 0.062\n",
      "Attended Music: 0.057\n",
      "Unattended Music: -0.016\n",
      "Split 2\n",
      "Speech-Trained Model\n",
      "Train: 0.192\n",
      "Attended Speech: 0.155\n",
      "Unattended Speech: -0.058\n",
      "Attended Music: 0.09\n",
      "Unattended Music: -0.087\n",
      "Music-Trained Model\n",
      "Train: 0.136\n",
      "Attended Speech: 0.006\n",
      "Unattended Speech: 0.036\n",
      "Attended Music: 0.17\n",
      "Unattended Music: 0.003\n",
      "Split 3\n",
      "Speech-Trained Model\n",
      "Train: 0.192\n",
      "Attended Speech: 0.152\n",
      "Unattended Speech: 0.11\n",
      "Attended Music: -0.0\n",
      "Unattended Music: 0.008\n",
      "Music-Trained Model\n",
      "Train: 0.144\n",
      "Attended Speech: 0.021\n",
      "Unattended Speech: -0.016\n",
      "Attended Music: 0.013\n",
      "Unattended Music: 0.052\n",
      "Split 4\n",
      "Speech-Trained Model\n",
      "Train: 0.197\n",
      "Attended Speech: -0.09\n",
      "Unattended Speech: 0.14\n",
      "Attended Music: -0.065\n",
      "Unattended Music: 0.036\n",
      "Music-Trained Model\n",
      "Train: 0.139\n",
      "Attended Speech: 0.004\n",
      "Unattended Speech: -0.018\n",
      "Attended Music: 0.097\n",
      "Unattended Music: -0.002\n",
      "Split 5\n",
      "Speech-Trained Model\n",
      "Train: 0.195\n",
      "Attended Speech: 0.034\n",
      "Unattended Speech: 0.017\n",
      "Attended Music: -0.035\n",
      "Unattended Music: -0.076\n",
      "Music-Trained Model\n",
      "Train: 0.137\n",
      "Attended Speech: 0.058\n",
      "Unattended Speech: 0.027\n",
      "Attended Music: 0.022\n",
      "Unattended Music: 0.043\n",
      "Split 6\n",
      "Speech-Trained Model\n",
      "Train: 0.195\n",
      "Attended Speech: 0.01\n",
      "Unattended Speech: 0.031\n",
      "Attended Music: -0.021\n",
      "Unattended Music: 0.018\n",
      "Music-Trained Model\n",
      "Train: 0.139\n",
      "Attended Speech: 0.031\n",
      "Unattended Speech: -0.015\n",
      "Attended Music: 0.052\n",
      "Unattended Music: 0.033\n",
      "Split 7\n",
      "Speech-Trained Model\n",
      "Train: 0.192\n",
      "Attended Speech: 0.164\n",
      "Unattended Speech: -0.011\n",
      "Attended Music: 0.173\n",
      "Unattended Music: 0.133\n",
      "Music-Trained Model\n",
      "Train: 0.136\n",
      "Attended Speech: -0.021\n",
      "Unattended Speech: -0.016\n",
      "Attended Music: 0.145\n",
      "Unattended Music: 0.097\n",
      "Split 8\n",
      "Speech-Trained Model\n",
      "Train: 0.195\n",
      "Attended Speech: 0.005\n",
      "Unattended Speech: 0.051\n",
      "Attended Music: 0.078\n",
      "Unattended Music: 0.081\n",
      "Music-Trained Model\n",
      "Train: 0.14\n",
      "Attended Speech: 0.019\n",
      "Unattended Speech: 0.035\n",
      "Attended Music: 0.006\n",
      "Unattended Music: 0.082\n",
      "Split 9\n",
      "Speech-Trained Model\n",
      "Train: 0.192\n",
      "Attended Speech: 0.179\n",
      "Unattended Speech: 0.347\n",
      "Attended Music: -0.066\n",
      "Unattended Music: 0.045\n",
      "Music-Trained Model\n",
      "Train: 0.138\n",
      "Attended Speech: -0.018\n",
      "Unattended Speech: 0.073\n",
      "Attended Music: -0.002\n",
      "Unattended Music: 0.074\n",
      "Split 10\n",
      "Speech-Trained Model\n",
      "Train: 0.192\n",
      "Attended Speech: 0.116\n",
      "Unattended Speech: 0.094\n",
      "Attended Music: -0.036\n",
      "Unattended Music: 0.033\n",
      "Music-Trained Model\n",
      "Train: 0.143\n",
      "Attended Speech: 0.012\n",
      "Unattended Speech: 0.002\n",
      "Attended Music: 0.058\n",
      "Unattended Music: 0.099\n",
      "Split 11\n",
      "Speech-Trained Model\n",
      "Train: 0.193\n",
      "Attended Speech: 0.068\n",
      "Unattended Speech: 0.03\n",
      "Attended Music: 0.009\n",
      "Unattended Music: 0.039\n",
      "Music-Trained Model\n",
      "Train: 0.139\n",
      "Attended Speech: 0.116\n",
      "Unattended Speech: 0.025\n",
      "Attended Music: 0.097\n",
      "Unattended Music: 0.084\n",
      "Split 12\n",
      "Speech-Trained Model\n",
      "Train: 0.19\n",
      "Attended Speech: 0.321\n",
      "Unattended Speech: 0.022\n",
      "Attended Music: -0.027\n",
      "Unattended Music: 0.068\n",
      "Music-Trained Model\n",
      "Train: 0.141\n",
      "Attended Speech: -0.032\n",
      "Unattended Speech: -0.0\n",
      "Attended Music: 0.058\n",
      "Unattended Music: 0.05\n",
      "Split 13\n",
      "Speech-Trained Model\n",
      "Train: 0.194\n",
      "Attended Speech: 0.045\n",
      "Unattended Speech: -0.097\n",
      "Attended Music: 0.226\n",
      "Unattended Music: -0.138\n",
      "Music-Trained Model\n",
      "Train: 0.14\n",
      "Attended Speech: 0.05\n",
      "Unattended Speech: 0.035\n",
      "Attended Music: 0.101\n",
      "Unattended Music: 0.052\n",
      "Split 14\n",
      "Speech-Trained Model\n",
      "Train: 0.193\n",
      "Attended Speech: 0.12\n",
      "Unattended Speech: 0.146\n",
      "Attended Music: 0.029\n",
      "Unattended Music: -0.065\n",
      "Music-Trained Model\n",
      "Train: 0.139\n",
      "Attended Speech: -0.027\n",
      "Unattended Speech: 0.071\n",
      "Attended Music: 0.039\n",
      "Unattended Music: 0.086\n",
      "Split 15\n",
      "Speech-Trained Model\n",
      "Train: 0.194\n",
      "Attended Speech: 0.029\n",
      "Unattended Speech: -0.034\n",
      "Attended Music: 0.0\n",
      "Unattended Music: -0.013\n",
      "Music-Trained Model\n",
      "Train: 0.132\n",
      "Attended Speech: 0.054\n",
      "Unattended Speech: -0.037\n",
      "Attended Music: 0.103\n",
      "Unattended Music: 0.042\n",
      "Split 16\n",
      "Speech-Trained Model\n",
      "Train: 0.194\n",
      "Attended Speech: 0.066\n",
      "Unattended Speech: -0.006\n",
      "Attended Music: 0.159\n",
      "Unattended Music: 0.082\n",
      "Music-Trained Model\n",
      "Train: 0.138\n",
      "Attended Speech: 0.089\n",
      "Unattended Speech: 0.059\n",
      "Attended Music: 0.031\n",
      "Unattended Music: -0.012\n",
      "Split 17\n",
      "Speech-Trained Model\n",
      "Train: 0.193\n",
      "Attended Speech: 0.107\n",
      "Unattended Speech: 0.098\n",
      "Attended Music: 0.189\n",
      "Unattended Music: 0.046\n",
      "Music-Trained Model\n",
      "Train: 0.139\n",
      "Attended Speech: 0.014\n",
      "Unattended Speech: 0.013\n",
      "Attended Music: 0.059\n",
      "Unattended Music: 0.007\n",
      "Split 18\n",
      "Speech-Trained Model\n",
      "Train: 0.192\n",
      "Attended Speech: 0.13\n",
      "Unattended Speech: 0.242\n",
      "Attended Music: 0.052\n",
      "Unattended Music: -0.084\n",
      "Music-Trained Model\n",
      "Train: 0.139\n",
      "Attended Speech: 0.028\n",
      "Unattended Speech: 0.045\n",
      "Attended Music: 0.168\n",
      "Unattended Music: 0.028\n",
      "Split 19\n",
      "Speech-Trained Model\n",
      "Train: 0.191\n",
      "Attended Speech: 0.191\n",
      "Unattended Speech: 0.138\n",
      "Attended Music: 0.211\n",
      "Unattended Music: 0.096\n",
      "Music-Trained Model\n",
      "Train: 0.138\n",
      "Attended Speech: 0.038\n",
      "Unattended Speech: 0.083\n",
      "Attended Music: 0.125\n",
      "Unattended Music: 0.09\n",
      "Split 20\n",
      "Speech-Trained Model\n",
      "Train: 0.195\n",
      "Attended Speech: 0.012\n",
      "Unattended Speech: 0.007\n",
      "Attended Music: -0.12\n",
      "Unattended Music: 0.035\n",
      "Music-Trained Model\n",
      "Train: 0.141\n",
      "Attended Speech: 0.034\n",
      "Unattended Speech: 0.072\n",
      "Attended Music: 0.12\n",
      "Unattended Music: 0.016\n",
      "Split 21\n",
      "Speech-Trained Model\n",
      "Train: 0.192\n",
      "Attended Speech: 0.1\n",
      "Unattended Speech: 0.002\n",
      "Attended Music: 0.048\n",
      "Unattended Music: -0.032\n",
      "Music-Trained Model\n",
      "Train: 0.143\n",
      "Attended Speech: -0.07\n",
      "Unattended Speech: 0.053\n",
      "Attended Music: -0.034\n",
      "Unattended Music: 0.012\n",
      "Split 22\n",
      "Speech-Trained Model\n",
      "Train: 0.19\n",
      "Attended Speech: 0.247\n",
      "Unattended Speech: 0.18\n",
      "Attended Music: -0.049\n",
      "Unattended Music: 0.041\n",
      "Music-Trained Model\n",
      "Train: 0.138\n",
      "Attended Speech: 0.01\n",
      "Unattended Speech: 0.031\n",
      "Attended Music: 0.092\n",
      "Unattended Music: 0.006\n",
      "Split 23\n",
      "Speech-Trained Model\n",
      "Train: 0.194\n",
      "Attended Speech: 0.068\n",
      "Unattended Speech: 0.121\n",
      "Attended Music: 0.088\n",
      "Unattended Music: -0.148\n",
      "Music-Trained Model\n",
      "Train: 0.139\n",
      "Attended Speech: -0.015\n",
      "Unattended Speech: -0.04\n",
      "Attended Music: 0.022\n",
      "Unattended Music: 0.033\n",
      "Split 24\n",
      "Speech-Trained Model\n",
      "Train: 0.192\n",
      "Attended Speech: 0.091\n",
      "Unattended Speech: 0.189\n",
      "Attended Music: 0.101\n",
      "Unattended Music: -0.067\n",
      "Music-Trained Model\n",
      "Train: 0.138\n",
      "Attended Speech: -0.019\n",
      "Unattended Speech: 0.043\n",
      "Attended Music: 0.007\n",
      "Unattended Music: 0.02\n",
      "Split 25\n",
      "Speech-Trained Model\n",
      "Train: 0.196\n",
      "Attended Speech: -0.006\n",
      "Unattended Speech: 0.155\n",
      "Attended Music: 0.128\n",
      "Unattended Music: -0.329\n",
      "Music-Trained Model\n",
      "Train: 0.138\n",
      "Attended Speech: -0.037\n",
      "Unattended Speech: 0.052\n",
      "Attended Music: 0.137\n",
      "Unattended Music: -0.023\n",
      "Split 26\n",
      "Speech-Trained Model\n",
      "Train: 0.193\n",
      "Attended Speech: 0.089\n",
      "Unattended Speech: 0.176\n",
      "Attended Music: -0.034\n",
      "Unattended Music: 0.022\n",
      "Music-Trained Model\n",
      "Train: 0.137\n",
      "Attended Speech: 0.084\n",
      "Unattended Speech: 0.046\n",
      "Attended Music: 0.043\n",
      "Unattended Music: 0.016\n",
      "Split 27\n",
      "Speech-Trained Model\n",
      "Train: 0.191\n",
      "Attended Speech: 0.056\n",
      "Unattended Speech: -0.032\n",
      "Attended Music: 0.018\n",
      "Unattended Music: 0.014\n",
      "Music-Trained Model\n",
      "Train: 0.139\n",
      "Attended Speech: -0.043\n",
      "Unattended Speech: -0.039\n",
      "Attended Music: 0.025\n",
      "Unattended Music: 0.131\n",
      "Split 28\n",
      "Speech-Trained Model\n",
      "Train: 0.195\n",
      "Attended Speech: -0.001\n",
      "Unattended Speech: 0.096\n",
      "Attended Music: -0.039\n",
      "Unattended Music: 0.211\n",
      "Music-Trained Model\n",
      "Train: 0.137\n",
      "Attended Speech: 0.08\n",
      "Unattended Speech: -0.019\n",
      "Attended Music: 0.113\n",
      "Unattended Music: 0.033\n",
      "Split 29\n",
      "Speech-Trained Model\n",
      "Train: 0.192\n",
      "Attended Speech: 0.106\n",
      "Unattended Speech: 0.034\n",
      "Attended Music: 0.044\n",
      "Unattended Music: 0.007\n",
      "Music-Trained Model\n",
      "Train: 0.14\n",
      "Attended Speech: 0.047\n",
      "Unattended Speech: -0.028\n",
      "Attended Music: 0.011\n",
      "Unattended Music: -0.01\n",
      "Split 30\n",
      "Speech-Trained Model\n",
      "Train: 0.193\n",
      "Attended Speech: 0.104\n",
      "Unattended Speech: 0.006\n",
      "Attended Music: -0.098\n",
      "Unattended Music: -0.013\n",
      "Music-Trained Model\n",
      "Train: 0.139\n",
      "Attended Speech: 0.051\n",
      "Unattended Speech: 0.095\n",
      "Attended Music: 0.086\n",
      "Unattended Music: 0.008\n",
      "Split 31\n",
      "Speech-Trained Model\n",
      "Train: 0.193\n",
      "Attended Speech: 0.099\n",
      "Unattended Speech: 0.11\n",
      "Attended Music: 0.014\n",
      "Unattended Music: -0.128\n",
      "Music-Trained Model\n",
      "Train: 0.141\n",
      "Attended Speech: -0.027\n",
      "Unattended Speech: -0.034\n",
      "Attended Music: 0.117\n",
      "Unattended Music: 0.032\n",
      "Split 32\n",
      "Speech-Trained Model\n",
      "Train: 0.193\n",
      "Attended Speech: 0.087\n",
      "Unattended Speech: 0.156\n",
      "Attended Music: 0.004\n",
      "Unattended Music: 0.267\n",
      "Music-Trained Model\n",
      "Train: 0.143\n",
      "Attended Speech: -0.04\n",
      "Unattended Speech: 0.005\n",
      "Attended Music: 0.053\n",
      "Unattended Music: 0.054\n",
      "Split 33\n",
      "Speech-Trained Model\n",
      "Train: 0.195\n",
      "Attended Speech: -0.003\n",
      "Unattended Speech: 0.24\n",
      "Attended Music: -0.023\n",
      "Unattended Music: -0.05\n",
      "Music-Trained Model\n",
      "Train: 0.141\n",
      "Attended Speech: 0.027\n",
      "Unattended Speech: -0.043\n",
      "Attended Music: 0.086\n",
      "Unattended Music: 0.059\n",
      "Split 34\n",
      "Speech-Trained Model\n",
      "Train: 0.192\n",
      "Attended Speech: 0.143\n",
      "Unattended Speech: 0.089\n",
      "Attended Music: 0.104\n",
      "Unattended Music: -0.147\n",
      "Music-Trained Model\n",
      "Train: 0.136\n",
      "Attended Speech: 0.09\n",
      "Unattended Speech: -0.042\n",
      "Attended Music: 0.166\n",
      "Unattended Music: -0.021\n",
      "Split 35\n",
      "Speech-Trained Model\n",
      "Train: 0.192\n",
      "Attended Speech: 0.138\n",
      "Unattended Speech: 0.089\n",
      "Attended Music: 0.142\n",
      "Unattended Music: 0.042\n",
      "Music-Trained Model\n",
      "Train: 0.141\n",
      "Attended Speech: 0.114\n",
      "Unattended Speech: 0.047\n",
      "Attended Music: 0.071\n",
      "Unattended Music: 0.013\n",
      "Split 36\n",
      "Speech-Trained Model\n",
      "Train: 0.193\n",
      "Attended Speech: 0.054\n",
      "Unattended Speech: 0.011\n",
      "Attended Music: 0.163\n",
      "Unattended Music: 0.204\n",
      "Music-Trained Model\n",
      "Train: 0.141\n",
      "Attended Speech: -0.003\n",
      "Unattended Speech: -0.003\n",
      "Attended Music: 0.06\n",
      "Unattended Music: 0.01\n",
      "Split 37\n",
      "Speech-Trained Model\n",
      "Train: 0.191\n",
      "Attended Speech: 0.169\n",
      "Unattended Speech: 0.026\n",
      "Attended Music: 0.097\n",
      "Unattended Music: -0.234\n",
      "Music-Trained Model\n",
      "Train: 0.133\n",
      "Attended Speech: 0.035\n",
      "Unattended Speech: 0.08\n",
      "Attended Music: 0.095\n",
      "Unattended Music: 0.083\n",
      "Split 38\n",
      "Speech-Trained Model\n",
      "Train: 0.193\n",
      "Attended Speech: 0.079\n",
      "Unattended Speech: 0.038\n",
      "Attended Music: 0.041\n",
      "Unattended Music: -0.025\n",
      "Music-Trained Model\n",
      "Train: 0.142\n",
      "Attended Speech: 0.071\n",
      "Unattended Speech: 0.017\n",
      "Attended Music: 0.106\n",
      "Unattended Music: 0.011\n",
      "Split 39\n",
      "Speech-Trained Model\n",
      "Train: 0.193\n",
      "Attended Speech: 0.028\n",
      "Unattended Speech: 0.081\n",
      "Attended Music: -0.06\n",
      "Unattended Music: 0.036\n",
      "Music-Trained Model\n",
      "Train: 0.142\n",
      "Attended Speech: 0.146\n",
      "Unattended Speech: 0.033\n",
      "Attended Music: 0.033\n",
      "Unattended Music: 0.056\n",
      "Split 40\n",
      "Speech-Trained Model\n",
      "Train: 0.193\n",
      "Attended Speech: 0.071\n",
      "Unattended Speech: 0.037\n",
      "Attended Music: -0.079\n",
      "Unattended Music: -0.088\n",
      "Music-Trained Model\n",
      "Train: 0.141\n",
      "Attended Speech: 0.001\n",
      "Unattended Speech: 0.018\n",
      "Attended Music: 0.075\n",
      "Unattended Music: 0.023\n",
      "Split 41\n",
      "Speech-Trained Model\n",
      "Train: 0.189\n",
      "Attended Speech: 0.29\n",
      "Unattended Speech: 0.153\n",
      "Attended Music: -0.159\n",
      "Unattended Music: 0.079\n",
      "Music-Trained Model\n",
      "Train: 0.141\n",
      "Attended Speech: 0.107\n",
      "Unattended Speech: -0.018\n",
      "Attended Music: -0.002\n",
      "Unattended Music: -0.009\n",
      "Split 42\n",
      "Speech-Trained Model\n",
      "Train: 0.193\n",
      "Attended Speech: 0.033\n",
      "Unattended Speech: -0.176\n",
      "Attended Music: 0.097\n",
      "Unattended Music: 0.112\n",
      "Music-Trained Model\n",
      "Train: 0.146\n",
      "Attended Speech: -0.008\n",
      "Unattended Speech: 0.032\n",
      "Attended Music: 0.006\n",
      "Unattended Music: 0.021\n",
      "Split 43\n",
      "Speech-Trained Model\n",
      "Train: 0.194\n",
      "Attended Speech: 0.036\n",
      "Unattended Speech: 0.097\n",
      "Attended Music: 0.027\n",
      "Unattended Music: 0.071\n",
      "Music-Trained Model\n",
      "Train: 0.141\n",
      "Attended Speech: 0.056\n",
      "Unattended Speech: -0.022\n",
      "Attended Music: 0.021\n",
      "Unattended Music: 0.023\n",
      "Split 44\n",
      "Speech-Trained Model\n",
      "Train: 0.193\n",
      "Attended Speech: 0.066\n",
      "Unattended Speech: -0.047\n",
      "Attended Music: -0.075\n",
      "Unattended Music: 0.122\n",
      "Music-Trained Model\n",
      "Train: 0.142\n",
      "Attended Speech: 0.048\n",
      "Unattended Speech: 0.03\n",
      "Attended Music: 0.04\n",
      "Unattended Music: 0.075\n",
      "Split 45\n",
      "Speech-Trained Model\n",
      "Train: 0.192\n",
      "Attended Speech: 0.123\n",
      "Unattended Speech: -0.0\n",
      "Attended Music: -0.028\n",
      "Unattended Music: -0.006\n",
      "Music-Trained Model\n",
      "Train: 0.144\n",
      "Attended Speech: 0.048\n",
      "Unattended Speech: -0.055\n",
      "Attended Music: 0.016\n",
      "Unattended Music: 0.003\n",
      "Split 46\n",
      "Speech-Trained Model\n",
      "Train: 0.193\n",
      "Attended Speech: 0.119\n",
      "Unattended Speech: 0.31\n",
      "Attended Music: 0.059\n",
      "Unattended Music: 0.048\n",
      "Music-Trained Model\n",
      "Train: 0.143\n",
      "Attended Speech: 0.064\n",
      "Unattended Speech: 0.03\n",
      "Attended Music: 0.058\n",
      "Unattended Music: 0.009\n",
      "Split 47\n",
      "Speech-Trained Model\n",
      "Train: 0.19\n",
      "Attended Speech: 0.244\n",
      "Unattended Speech: 0.152\n",
      "Attended Music: 0.006\n",
      "Unattended Music: -0.055\n",
      "Music-Trained Model\n",
      "Train: 0.143\n",
      "Attended Speech: 0.125\n",
      "Unattended Speech: 0.092\n",
      "Attended Music: 0.126\n",
      "Unattended Music: 0.027\n",
      "Split 48\n",
      "Speech-Trained Model\n",
      "Train: 0.192\n",
      "Attended Speech: 0.148\n",
      "Unattended Speech: 0.084\n",
      "Attended Music: -0.017\n",
      "Unattended Music: -0.158\n",
      "Music-Trained Model\n",
      "Train: 0.14\n",
      "Attended Speech: 0.003\n",
      "Unattended Speech: 0.028\n",
      "Attended Music: 0.059\n",
      "Unattended Music: 0.017\n",
      "Split 49\n",
      "Speech-Trained Model\n",
      "Train: 0.192\n",
      "Attended Speech: 0.179\n",
      "Unattended Speech: 0.095\n",
      "Attended Music: 0.059\n",
      "Unattended Music: -0.106\n",
      "Music-Trained Model\n",
      "Train: 0.138\n",
      "Attended Speech: -0.038\n",
      "Unattended Speech: 0.04\n",
      "Attended Music: -0.008\n",
      "Unattended Music: 0.034\n",
      "Split 50\n",
      "Speech-Trained Model\n",
      "Train: 0.193\n",
      "Attended Speech: 0.13\n",
      "Unattended Speech: -0.099\n",
      "Attended Music: 0.001\n",
      "Unattended Music: 0.008\n",
      "Music-Trained Model\n",
      "Train: 0.139\n",
      "Attended Speech: 0.047\n",
      "Unattended Speech: 0.017\n",
      "Attended Music: 0.07\n",
      "Unattended Music: 0.071\n",
      "Split 51\n",
      "Speech-Trained Model\n",
      "Train: 0.194\n",
      "Attended Speech: 0.019\n",
      "Unattended Speech: 0.038\n",
      "Attended Music: -0.13\n",
      "Unattended Music: 0.037\n",
      "Music-Trained Model\n",
      "Train: 0.14\n",
      "Attended Speech: 0.063\n",
      "Unattended Speech: 0.028\n",
      "Attended Music: 0.028\n",
      "Unattended Music: -0.075\n",
      "Split 52\n",
      "Speech-Trained Model\n",
      "Train: 0.191\n",
      "Attended Speech: 0.229\n",
      "Unattended Speech: 0.085\n",
      "Attended Music: 0.223\n",
      "Unattended Music: -0.035\n",
      "Music-Trained Model\n",
      "Train: 0.139\n",
      "Attended Speech: 0.04\n",
      "Unattended Speech: 0.01\n",
      "Attended Music: 0.054\n",
      "Unattended Music: -0.053\n",
      "Split 53\n",
      "Speech-Trained Model\n",
      "Train: 0.191\n",
      "Attended Speech: 0.164\n",
      "Unattended Speech: 0.068\n",
      "Attended Music: -0.022\n",
      "Unattended Music: -0.09\n",
      "Music-Trained Model\n",
      "Train: 0.14\n",
      "Attended Speech: -0.03\n",
      "Unattended Speech: 0.064\n",
      "Attended Music: 0.068\n",
      "Unattended Music: -0.001\n",
      "Split 54\n",
      "Speech-Trained Model\n",
      "Train: 0.191\n",
      "Attended Speech: 0.07\n",
      "Unattended Speech: 0.053\n",
      "Attended Music: 0.107\n",
      "Unattended Music: -0.026\n",
      "Music-Trained Model\n",
      "Train: 0.139\n",
      "Attended Speech: 0.135\n",
      "Unattended Speech: -0.008\n",
      "Attended Music: 0.04\n",
      "Unattended Music: 0.054\n",
      "Split 55\n",
      "Speech-Trained Model\n",
      "Train: 0.193\n",
      "Attended Speech: 0.09\n",
      "Unattended Speech: 0.069\n",
      "Attended Music: -0.053\n",
      "Unattended Music: -0.012\n",
      "Music-Trained Model\n",
      "Train: 0.141\n",
      "Attended Speech: 0.004\n",
      "Unattended Speech: -0.016\n",
      "Attended Music: -0.009\n",
      "Unattended Music: 0.031\n",
      "Split 56\n",
      "Speech-Trained Model\n",
      "Train: 0.194\n",
      "Attended Speech: 0.042\n",
      "Unattended Speech: 0.166\n",
      "Attended Music: -0.032\n",
      "Unattended Music: 0.114\n",
      "Music-Trained Model\n",
      "Train: 0.14\n",
      "Attended Speech: 0.112\n",
      "Unattended Speech: 0.046\n",
      "Attended Music: 0.057\n",
      "Unattended Music: -0.041\n",
      "Split 57\n",
      "Speech-Trained Model\n",
      "Train: 0.193\n",
      "Attended Speech: 0.101\n",
      "Unattended Speech: 0.247\n",
      "Attended Music: -0.119\n",
      "Unattended Music: -0.074\n",
      "Music-Trained Model\n",
      "Train: 0.141\n",
      "Attended Speech: 0.061\n",
      "Unattended Speech: -0.005\n",
      "Attended Music: 0.043\n",
      "Unattended Music: -0.014\n",
      "Split 58\n",
      "Speech-Trained Model\n",
      "Train: 0.192\n",
      "Attended Speech: 0.064\n",
      "Unattended Speech: 0.166\n",
      "Attended Music: -0.13\n",
      "Unattended Music: -0.058\n",
      "Music-Trained Model\n",
      "Train: 0.137\n",
      "Attended Speech: 0.019\n",
      "Unattended Speech: 0.089\n",
      "Attended Music: 0.087\n",
      "Unattended Music: 0.005\n",
      "Split 59\n",
      "Speech-Trained Model\n",
      "Train: 0.193\n",
      "Attended Speech: 0.126\n",
      "Unattended Speech: 0.075\n",
      "Attended Music: 0.024\n",
      "Unattended Music: 0.079\n",
      "Music-Trained Model\n",
      "Train: 0.141\n",
      "Attended Speech: -0.011\n",
      "Unattended Speech: 0.1\n",
      "Attended Music: 0.099\n",
      "Unattended Music: 0.065\n",
      "Split 60\n",
      "Speech-Trained Model\n",
      "Train: 0.194\n",
      "Attended Speech: 0.051\n",
      "Unattended Speech: 0.064\n",
      "Attended Music: 0.106\n",
      "Unattended Music: 0.06\n",
      "Music-Trained Model\n",
      "Train: 0.138\n",
      "Attended Speech: 0.046\n",
      "Unattended Speech: 0.056\n",
      "Attended Music: 0.094\n",
      "Unattended Music: 0.047\n",
      "Speech-Trained Model\n",
      "Average Training Correlation: 0.19285386153820155\n",
      "Average Attended Speech Test Correlation: 0.09819108211845311\n",
      "Average Unttended Speech Test Correlation: 0.07966482274582047\n",
      "Average Attended Music Test Correlation: 0.025128362025130038\n",
      "Average Unttended Music Test Correlation: -0.0007487173307077442\n",
      "Music-Trained Model\n",
      "Average Training Correlation: 0.13964738025112222\n",
      "Average Attended Speech Test Correlation: 0.03195249499839805\n",
      "Average Unttended Speech Test Correlation: 0.022407231767759512\n",
      "Average Attended Music Test Correlation: 0.06378627849363341\n",
      "Average Unttended Music Test Correlation: 0.0290220848609364\n"
     ]
    }
   ],
   "source": [
    "speech_model_train_corrs = []\n",
    "speech_model_speech_att_test_corrs = []\n",
    "speech_model_speech_unatt_test_corrs = []\n",
    "speech_model_music_att_test_corrs = []\n",
    "speech_model_music_unatt_test_corrs = []\n",
    "\n",
    "music_model_train_corrs = []\n",
    "music_model_speech_att_test_corrs = []\n",
    "music_model_speech_unatt_test_corrs = []\n",
    "music_model_music_att_test_corrs = []\n",
    "music_model_music_unatt_test_corrs = []\n",
    "\n",
    "speech_sample_len = speech_eeg.shape[0]\n",
    "music_sample_len = music_eeg.shape[0]\n",
    "\n",
    "k_cv = 60\n",
    "for i in range(k_cv):\n",
    "    print(f'Split {i+1}')\n",
    "\n",
    "    #Train Test Split\n",
    "    \n",
    "    speech_eeg_test = speech_eeg[i*(round(speech_sample_len/k_cv)):(i+1)*(round(speech_sample_len/k_cv)),:]\n",
    "    speech_stim_att_test = speech_stim_att[i*(round(speech_sample_len/k_cv)):(i+1)*(round(speech_sample_len/k_cv)),:]\n",
    "    speech_stim_unatt_test = speech_stim_unatt[i*(round(speech_sample_len/k_cv)):(i+1)*(round(speech_sample_len/k_cv)),:]\n",
    "\n",
    "    music_eeg_test = music_eeg[i*(round(music_sample_len/k_cv)):(i+1)*(round(music_sample_len/k_cv)),:]\n",
    "    music_stim_att_test = music_stim_att[i*(round(music_sample_len/k_cv)):(i+1)*(round(music_sample_len/k_cv)),:]\n",
    "    music_stim_unatt_test = music_stim_unatt[i*(round(music_sample_len/k_cv)):(i+1)*(round(music_sample_len/k_cv)),:]\n",
    "\n",
    "    speech_eeg_train = np.concatenate((speech_eeg[:i*(round(speech_sample_len/k_cv)),:],speech_eeg[(i+1)*(round(speech_sample_len/k_cv)):,:]),axis=0)\n",
    "    speech_stim_train = np.concatenate((speech_stim_att[:i*(round(speech_sample_len/k_cv)),:],speech_stim_att[(i+1)*(round(speech_sample_len/k_cv)):,:]),axis=0)\n",
    "\n",
    "    music_eeg_train = np.concatenate((music_eeg[:i*(round(music_sample_len/k_cv)),:],music_eeg[(i+1)*(round(music_sample_len/k_cv)):,:]),axis=0)\n",
    "    music_stim_train = np.concatenate((music_stim_att[:i*(round(music_sample_len/k_cv)),:],music_stim_att[(i+1)*(round(music_sample_len/k_cv)):,:]),axis=0)\n",
    "    \n",
    "    #Lags\n",
    "    \n",
    "    speech_eeg_train = lag_generator_new(speech_eeg_train,lags_neuro)\n",
    "    speech_stim_train = lag_generator_new(speech_stim_train,lags_stim)\n",
    "\n",
    "    music_eeg_train = lag_generator_new(music_eeg_train,lags_neuro)\n",
    "    music_stim_train = lag_generator_new(music_stim_train,lags_stim)\n",
    "    \n",
    "    speech_eeg_test = lag_generator_new(speech_eeg_test,lags_neuro)\n",
    "    speech_stim_att_test = lag_generator_new(speech_stim_att_test,lags_stim)\n",
    "    speech_stim_unatt_test = lag_generator_new(speech_stim_unatt_test,lags_stim)\n",
    "\n",
    "    music_eeg_test = lag_generator_new(music_eeg_test,lags_neuro)\n",
    "    music_stim_att_test = lag_generator_new(music_stim_att_test,lags_stim)\n",
    "    music_stim_unatt_test = lag_generator_new(music_stim_unatt_test,lags_stim)\n",
    "\n",
    "    #Training\n",
    "    \n",
    "    cca_speech_att = CCA(n_components=3)\n",
    "    cca_speech_att = cca_speech_att.fit(speech_eeg_train, speech_stim_train)\n",
    "\n",
    "    cca_music_att = CCA(n_components=3)\n",
    "    cca_music_att = cca_music_att.fit(music_eeg_train, music_stim_train)\n",
    "\n",
    "    #Evaluations\n",
    "\n",
    "    print(\"Speech-Trained Model\")\n",
    "    \n",
    "    X_c, Y_c = cca_speech_att.transform(speech_eeg_train, speech_stim_train)\n",
    "    r_fwd = pearsonr(np.squeeze(X_c.flatten()), np.squeeze(Y_c.flatten())).statistic\n",
    "    print(f\"Train: {r_fwd.round(3)}\")\n",
    "    speech_model_train_corrs.append(r_fwd)\n",
    "\n",
    "    X_c, Y_c = cca_speech_att.transform(speech_eeg_test, speech_stim_att_test)\n",
    "    r_fwd = pearsonr(np.squeeze(X_c.flatten()), np.squeeze(Y_c.flatten())).statistic\n",
    "    print(f\"Attended Speech: {r_fwd.round(3)}\")\n",
    "    speech_model_speech_att_test_corrs.append(r_fwd)\n",
    "    \n",
    "    X_c, Y_c = cca_speech_att.transform(music_eeg_test, music_stim_unatt_test)\n",
    "    r_fwd = pearsonr(np.squeeze(X_c.flatten()), np.squeeze(Y_c.flatten())).statistic\n",
    "    print(f\"Unattended Speech: {r_fwd.round(3)}\")\n",
    "    speech_model_music_unatt_test_corrs.append(r_fwd)\n",
    "\n",
    "    X_c, Y_c = cca_speech_att.transform(music_eeg_test, music_stim_att_test)\n",
    "    r_fwd = pearsonr(np.squeeze(X_c.flatten()), np.squeeze(Y_c.flatten())).statistic\n",
    "    print(f\"Attended Music: {r_fwd.round(3)}\")\n",
    "    speech_model_music_att_test_corrs.append(r_fwd)\n",
    "\n",
    "    X_c, Y_c = cca_speech_att.transform(speech_eeg_test, speech_stim_unatt_test)\n",
    "    r_fwd = pearsonr(np.squeeze(X_c.flatten()), np.squeeze(Y_c.flatten())).statistic\n",
    "    print(f\"Unattended Music: {r_fwd.round(3)}\")\n",
    "    speech_model_speech_unatt_test_corrs.append(r_fwd)\n",
    "\n",
    "\n",
    "    print(\"Music-Trained Model\")\n",
    "    \n",
    "    X_c, Y_c = cca_music_att.transform(music_eeg_train, music_stim_train)\n",
    "    r_fwd = pearsonr(np.squeeze(X_c.flatten()), np.squeeze(Y_c.flatten())).statistic\n",
    "    print(f\"Train: {r_fwd.round(3)}\")\n",
    "    music_model_train_corrs.append(r_fwd)\n",
    "\n",
    "    X_c, Y_c = cca_music_att.transform(speech_eeg_test, speech_stim_att_test)\n",
    "    r_fwd = pearsonr(np.squeeze(X_c.flatten()), np.squeeze(Y_c.flatten())).statistic\n",
    "    print(f\"Attended Speech: {r_fwd.round(3)}\")\n",
    "    music_model_speech_att_test_corrs.append(r_fwd)\n",
    "    \n",
    "    X_c, Y_c = cca_music_att.transform(music_eeg_test, music_stim_unatt_test)\n",
    "    r_fwd = pearsonr(np.squeeze(X_c.flatten()), np.squeeze(Y_c.flatten())).statistic\n",
    "    print(f\"Unattended Speech: {r_fwd.round(3)}\")\n",
    "    music_model_music_unatt_test_corrs.append(r_fwd)\n",
    "\n",
    "    X_c, Y_c = cca_music_att.transform(music_eeg_test, music_stim_att_test)\n",
    "    r_fwd = pearsonr(np.squeeze(X_c.flatten()), np.squeeze(Y_c.flatten())).statistic\n",
    "    print(f\"Attended Music: {r_fwd.round(3)}\")\n",
    "    music_model_music_att_test_corrs.append(r_fwd)\n",
    "\n",
    "    X_c, Y_c = cca_music_att.transform(speech_eeg_test, speech_stim_unatt_test)\n",
    "    r_fwd = pearsonr(np.squeeze(X_c.flatten()), np.squeeze(Y_c.flatten())).statistic\n",
    "    print(f\"Unattended Music: {r_fwd.round(3)}\")\n",
    "    music_model_speech_unatt_test_corrs.append(r_fwd)\n",
    "\n",
    "print(\"Speech-Trained Model\")\n",
    "print(f'Average Training Correlation: {np.mean(speech_model_train_corrs)}')\n",
    "print(f'Average Attended Speech Test Correlation: {np.mean(speech_model_speech_att_test_corrs)}')\n",
    "print(f'Average Unttended Speech Test Correlation: {np.mean(speech_model_music_unatt_test_corrs)}')\n",
    "print(f'Average Attended Music Test Correlation: {np.mean(speech_model_music_att_test_corrs)}')\n",
    "print(f'Average Unttended Music Test Correlation: {np.mean(speech_model_speech_unatt_test_corrs)}')\n",
    "\n",
    "print(\"Music-Trained Model\")\n",
    "print(f'Average Training Correlation: {np.mean(music_model_train_corrs)}')\n",
    "print(f'Average Attended Speech Test Correlation: {np.mean(music_model_speech_att_test_corrs)}')\n",
    "print(f'Average Unttended Speech Test Correlation: {np.mean(music_model_music_unatt_test_corrs)}')\n",
    "print(f'Average Attended Music Test Correlation: {np.mean(music_model_music_att_test_corrs)}')\n",
    "print(f'Average Unttended Music Test Correlation: {np.mean(music_model_speech_unatt_test_corrs)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "31e03fd5-2510-421f-ada6-723d29f04c4d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Speech-Trained Model\n",
      "Speech-Attended Music-Unattended AAD Accuracy: 0.75\n",
      "Music-Attended Speech-Unattended AAD Accuracy: 0.35\n",
      "Music-Trained Model\n",
      "Speech-Attended Music-Unattended AAD Accuracy: 0.48333333333333334\n",
      "Music-Attended Speech-Unattended AAD Accuracy: 0.75\n",
      "Average\n",
      "Speech-Attended Music-Unattended AAD Accuracy: 0.75\n",
      "Music-Attended Speech-Unattended AAD Accuracy: 0.43333333333333335\n",
      "Weighted Average\n",
      "Weight: 3.5\n",
      "Speech-Attended Music-Unattended AAD Accuracy: 0.6833333333333333\n",
      "Music-Attended Speech-Unattended AAD Accuracy: 0.6166666666666667\n"
     ]
    }
   ],
   "source": [
    "print(\"Speech-Trained Model\")\n",
    "print(f\"Speech-Attended Music-Unattended AAD Accuracy: {np.mean([True if speech_model_speech_att_test_corrs[i] > speech_model_speech_unatt_test_corrs[i] else False for i in range(len(speech_model_speech_att_test_corrs))])}\")\n",
    "print(f\"Music-Attended Speech-Unattended AAD Accuracy: {np.mean([True if speech_model_music_att_test_corrs[i] > speech_model_music_unatt_test_corrs[i] else False for i in range(len(speech_model_music_att_test_corrs))])}\")\n",
    "\n",
    "print(\"Music-Trained Model\")\n",
    "print(f\"Speech-Attended Music-Unattended AAD Accuracy: {np.mean([True if music_model_speech_att_test_corrs[i] > music_model_speech_unatt_test_corrs[i] else False for i in range(len(music_model_speech_att_test_corrs))])}\")\n",
    "print(f\"Music-Attended Speech-Unattended AAD Accuracy: {np.mean([True if music_model_music_att_test_corrs[i] > music_model_music_unatt_test_corrs[i] else False for i in range(len(music_model_music_att_test_corrs))])}\")\n",
    "\n",
    "print(\"Average\")\n",
    "print(f\"Speech-Attended Music-Unattended AAD Accuracy: {np.mean([True if speech_model_speech_att_test_corrs[i] + music_model_speech_att_test_corrs[i] > speech_model_speech_unatt_test_corrs[i] + music_model_speech_unatt_test_corrs[i] else False for i in range(len(music_model_speech_att_test_corrs))])}\")\n",
    "print(f\"Music-Attended Speech-Unattended AAD Accuracy: {np.mean([True if speech_model_music_att_test_corrs[i] + music_model_music_att_test_corrs[i] > speech_model_music_unatt_test_corrs[i] + music_model_music_unatt_test_corrs[i] else False for i in range(len(music_model_music_att_test_corrs))])}\")\n",
    "\n",
    "print(\"Weighted Average\")\n",
    "weight = 3.5\n",
    "print(f\"Weight: {weight}\")\n",
    "print(f\"Speech-Attended Music-Unattended AAD Accuracy: {np.mean([True if speech_model_speech_att_test_corrs[i] + music_model_speech_att_test_corrs[i] * weight > speech_model_speech_unatt_test_corrs[i] + music_model_speech_unatt_test_corrs[i] * weight else False for i in range(len(music_model_speech_att_test_corrs))])}\")\n",
    "print(f\"Music-Attended Speech-Unattended AAD Accuracy: {np.mean([True if speech_model_music_att_test_corrs[i] + music_model_music_att_test_corrs[i] * weight > speech_model_music_unatt_test_corrs[i] + music_model_music_unatt_test_corrs[i] * weight else False for i in range(len(music_model_music_att_test_corrs))])}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (cu)",
   "language": "python",
   "name": "cu"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
